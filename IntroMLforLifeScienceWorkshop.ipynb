{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "B0eQobFj9Sk7"
      },
      "source": [
        "# Workshop -- Machine learning in life sciences\n",
        "### What is it, when should it be used and how to avoid common pitfalls\n",
        "\n",
        "**Author:** Benjamin Goudey, Research Fellow in Florey Department of Neuroscience and Mental Health at The University of Melbourne\n",
        "**Last updated:** 1/6/2023 "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Gyo-XHug9SlA"
      },
      "source": [
        "# Introduction\n",
        "Welcome to the workshop! This notebook accompanies the workshop *Applying machine learning in life sciences: what does it mean and how to avoid common traps.*\n",
        "\n",
        "The notebook focuses on the problem of predicting diabetes from a few clinical and blood measurements, as well as several \"noise\" variables.\n",
        "\n",
        "The notebook is split into into four sections. \n",
        "\n",
        "0. Welcome and setup\n",
        "1. Exploring the data and fitting a model and measuring performance\n",
        "2. Pitfall 1: Evaluation frameworks and generalisation\n",
        "3.  Pitfall 2: Selecting features and model parameters\n",
        "\n",
        "There will be a number of models, measures and algorithms that will be used and will be briefly explained in the accompanying tutorial but will not be covered in detail. The skikit-learn documentation will be valuable here (https://scikit-learn.org/stable/modules/classes.html)\n",
        "\n",
        "The notebook assumes familiarity with python, and a passing familiarity with the pandas, matplotlib/seaborn and numpy/scipy packages. But even if you don't have this, the idea is that this notebook should help you get an idea of some of the concepts around machine learning and may be a useful resource for you at some stage. \n",
        "\n",
        "PLease note: the expectation is you should be able to follow along rather than write this code from scratch. You should be able to run each cell in the notebook to get an output and then comments should direct you to indicate which parameters to change. If you get stuck, let us know!"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "odlRpgfkM282"
      },
      "source": [
        "# 0 Notebook Setup \n",
        "\n",
        "Don't worry too much about the code in this section. We load in the necessary packages and then there are a number of functions to load in the data or plot outputs. The details of these are mostly not needed for this workshop and I'll step through in more detail when they are needed. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75-A4WtJM8fi"
      },
      "source": [
        "## 0.1 Install and load required packages"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "**Warning:** you may need to run this cell twice\n",
        "\n",
        "It installs a recent version of ydata-profiling, a tool for summarising the content of a pandas data frame. When the install occurs, a little reset button may appear. Press this and re-run the cell to load all packages into your environment.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vFSeoWp_9SlC"
      },
      "outputs": [],
      "source": [
        "!pip install ydata_profiling \n",
        "# Load in everything we need\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.stats import multivariate_normal as mvn\n",
        "\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, RepeatedKFold, KFold, GridSearchCV\n",
        "from sklearn import datasets\n",
        "from sklearn import metrics\n",
        "from sklearn import feature_selection\n",
        "from sklearn import linear_model\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "import graphviz \n",
        "\n",
        "# Models\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression, Lasso\n",
        "\n",
        "# Plotting\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Pandas-profiling - generates an interactive report\n",
        "from ydata_profiling import ProfileReport\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QWvjl7a9SlJ"
      },
      "source": [
        "## 0.2 Loading helper function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kKGx6iUV9SlJ"
      },
      "outputs": [],
      "source": [
        "#\n",
        "\n",
        "# Add some random noise to a given data.frame\n",
        "def append_noise(df, n_noise=20):\n",
        "    noise_measures = np.random.rand(df.shape[0],n_noise)\n",
        "    noise_measures = StandardScaler().fit(noise_measures).transform(noise_measures)\n",
        "    noise_names = [\"noise_{}\".format(i) for i in range(n_noise)]\n",
        "    noise_df = pd.DataFrame(data=noise_measures,columns=noise_names)\n",
        "    return(pd.concat([df,noise_df], axis=1))\n",
        "\n",
        "# Change the target to be binary\n",
        "def binarize_y(y):     \n",
        "    return(1*(y>100))\n",
        "\n",
        "# Rename a number of columns to make them more describptive\n",
        "def tidy_diabetes_names(X):     \n",
        "    # We rename the variables to be more descriptive\n",
        "    X.rename({'s1': 'tc', \n",
        "              's2': 'ldl', \n",
        "              's3': 'hdl',\n",
        "              's4': 'tch',\n",
        "              's5': 'ltg',\n",
        "              's6': 'glu',}\n",
        "             , axis=1, inplace=True)\n",
        "    return(X)\n",
        "\n",
        "\n",
        "#\n",
        "#\n",
        "# Generate a new dataset based on the existing diabetes dataset\n",
        "def generate_novel_data(n_samples=1000, n_noise=20):\n",
        "\n",
        "    # Reload the diabetes dataset to retrieve the continuous progression score    \n",
        "    X,y=datasets.load_diabetes(as_frame=True, return_X_y=True)\n",
        "    \n",
        "    # We rename the variables to be more descriptive\n",
        "    X = tidy_diabetes_names(X)\n",
        "\n",
        "    # Combine X and y into a single data.frame\n",
        "    X_y = X.assign(y = y)\n",
        "    \n",
        "    # Based on existing data means and covariance, generate some new data \n",
        "    # (assumes mutlivariate normal, unlikely to be true but good enough)\n",
        "    # Then turn into a data.frame\n",
        "    sim_dat = mvn.rvs(mean = X_y.mean(), cov=X_y.cov(), size = n_samples)\n",
        "    sim_X_y = pd.DataFrame(data = sim_dat, columns = X_y.columns)\n",
        "\n",
        "    # Split back in to X and y\n",
        "    sim_y = binarize_y(sim_X_y['y'])\n",
        "    sim_X = sim_X_y.drop(columns=['y'])\n",
        "    \n",
        "    #If we've specified, add some number of randomly generated features   \n",
        "    if n_noise>0:\n",
        "        sim_X=append_noise(sim_X, n_noise)\n",
        "    \n",
        "    return(sim_X, sim_y)\n",
        "    \n",
        "    \n",
        "#\n",
        "# Load in the diabetes dataset that comes with sklearn. \n",
        "# Then tidy names, binaryise the progression variable and add some noise (if specified)\n",
        "def load_diabetes_data(n_noise=20):\n",
        "    # Load in the data as a data.frame, split into X (features) and y (target) \n",
        "    X,y=datasets.load_diabetes(as_frame=True, return_X_y=True)\n",
        "    \n",
        "    # The defaul names are a bit strange so we clean then up\n",
        "    X = tidy_diabetes_names(X)\n",
        "    \n",
        "    # Target variable is a quantitative measure of disease progression one year after baseline\n",
        "    # that we dichotimise (turn into two groups) to simplify some analysis. \n",
        "    y = binarize_y(y)\n",
        "\n",
        "    #If we've specified, add some number of randomly generated features\n",
        "    if n_noise>0:\n",
        "        X=append_noise(X, n_noise)\n",
        "    \n",
        "    X_sim, y_sim = generate_novel_data(n_samples=1000, n_noise=n_noise)\n",
        "    \n",
        "    return (X, y, X_sim, y_sim)\n",
        "\n",
        "# Plot a ROC curve with a label\n",
        "def plot_roc(y, yp, label=\"\", ax=None):\n",
        "    fpr, tpr, thresh = metrics.roc_curve(y, yp)\n",
        "    auc = metrics.roc_auc_score(y, yp)\n",
        "    if ax:\n",
        "        ax.plot(fpr,tpr,label=\"{} AUC={:.2f}\".format(label, auc))\n",
        "    else:\n",
        "        plt.plot(fpr,tpr,label=\"{} AUC={:.2f}\".format(label, auc))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DY4rIR8e9SlO"
      },
      "source": [
        "# Session 1: Exploring the data and fitting a basic model\n",
        "\n",
        "Here, we will explore a given dataset related to diabetes and fit a basic model using the scikit-learn package.\n",
        "\n",
        "In particular, we aim to\n",
        " - understand the dataset, its variables and their relationship. \n",
        " - introduce the pandas-profiling and scikit-learn packages. \n",
        " - demonstrate how to fit a model using sklearn and look at the outputs. \n",
        "\n",
        "## Background: Dataset\n",
        "\n",
        "We will be making use of a real dataset of 442 diabetes patients as well as a simulated dataset derived from this data. This is relatively small in the machine learning space but is common (or even large) for many clinical/bioinformatics analyses. \n",
        "\n",
        "10 attributes have been measured, with variable names and descriptions given below:\n",
        "- *age*: age in years\n",
        "- *sex*: biological sex of the participant\n",
        "- *bmi*: body mass index\n",
        "- *bp*: average blood pressure\n",
        "- *tc*: total serum cholesterol\n",
        "- *ldl*: low-density lipoproteins\n",
        "- *hdl*: high-density lipoproteins\n",
        "- *tch*: total cholesterol / HDL\n",
        "- *ltg*: possibly log of serum triglycerides level\n",
        "- *glu*: blood sugar level\n",
        "\n",
        "Here, we predict a binary target indicating progression of diabetes after one year or not (1 or 0). This is derived from \"a quantitative measure of disease progression one year after baseline\" (Efron et al. (2004)) though it is unclear exactly what this measurement is. I've threshold this value at 100. \n",
        "\n",
        "\n",
        "We derive a simulated dataset from the original dataset which we treat as an external replication cohort. By default there are 1000 samples in this simulated data. \n",
        "\n",
        "### Analysis aim\n",
        "The analysis goals from this dataset are typical of a predictive task in this area: \n",
        ">Two hopes were evident [from the data], that the model would produce accurate baseline predictions of response for future patients and that the form of the model would suggest which covariates were important factors in disease progression.\n",
        "\n",
        "I'm going to assume a more specific question **\"do blood serum markers help predict diabetes progression beyond age, sex, bmi and blood glucose?\"**. Now we have a specific baseline we can evaluate against. \n",
        "\n",
        "Further information is available at https://scikit-learn.org/stable/datasets/toy_dataset.html#diabetes-dataset and data is taken from the original paper https://tibshirani.su.domains/ftp/lars.pdf.\n",
        "\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciSrEHOUNOa-"
      },
      "source": [
        "## 1.1 Load in the data\n",
        "We will load in a cleaned-up version of the dataset (using the *load_diabetes_data()* function)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LIpL_BVn9SlP",
        "outputId": "89c44e61-02a8-40c8-dbd2-75a5fd6362fa"
      },
      "outputs": [],
      "source": [
        "# This cell creates of the dataframe that has been entered. \n",
        "X, y, X_ext, y_ext = load_diabetes_data(n_noise=40)\n",
        "\n",
        "print(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwVH1-DB9SlR"
      },
      "source": [
        "## 1.2 Data exploration\n",
        "We'll begin by exploring the data that is available. While we have a description of the fields, understanding the relationships between individual features and their relationship with the outcome of interest is informative for helping to understand downstream. \n",
        "\n",
        "Rather than try to generate a bunch of plots manually, we can make use of a package called *pandas-profiling*, which provides a bunch of handy plots. *This will take a minute or two to run*. After that a small report below will be generated that provides an overview of the different features and their relationships. \n",
        "\n",
        "**Examine the following:**\n",
        "\n",
        "1. What are the different types of features - which are numerical? which are categorical? are any unclear?\n",
        "2. Which features are correlated with each other? How strong are these?\n",
        "3. Is there anything unexpected about the data?\n",
        "4. Is there any missing data?\n",
        "5. What does the target variable look like? What is its distribution? Are there any obvious relationships?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 871,
          "referenced_widgets": [
            "80ad4f7361e340e5a5ff87edb57c4bc5",
            "2210402b98fb4bf2a1627cba57c386e2",
            "1e075c7bb2b341fb9d3be3948631d9c6",
            "0e52c6cba9864d0f89c9171c038226e8",
            "e0b6ba0328cc49c19598d544f64a6e63",
            "748a8e9291584a6bb88da45a5977c4d8",
            "3990a336e1d14dcb9eb37f9dfc10a710",
            "88e9ee2dbb29440fa181bb656fc90407",
            "1b927521ecab43bc937c7fad732e4138",
            "d3fc7bdc02c0491c8e6b6498984fba6a",
            "c515e2a0d3c64229839059a29ac2db46",
            "54d8f558ba794237bee5256118ba0e00"
          ]
        },
        "id": "zY6W_IAa9SlS",
        "outputId": "f73a955e-5a58-4eb1-9d33-0bb29c0227a7"
      },
      "outputs": [],
      "source": [
        "# Ignore the noise features for this exploration\n",
        "X_y = X.iloc[:, 0:11].assign(y = y)\n",
        "profile = ProfileReport(X_y, \n",
        "                       correlations={\n",
        "        \"pearson\": {\"calculate\": False},\n",
        "        \"spearman\": {\"calculate\": True},\n",
        "        \"kendall\": {\"calculate\": False},\n",
        "        \"phi_k\": {\"calculate\": True},\n",
        "        \"cramers\": {\"calculate\": False},\n",
        "    })\n",
        "\n",
        "#profile.to_widgets()\n",
        "# The line above makes nice looking output but I think it is broken in either Google Colab or Python notebooks. \n",
        "profile.to_notebook_iframe()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "GeM844D09SlT"
      },
      "source": [
        "## 1.3 Fitting a model to the data\n",
        "\n",
        "Lets fit a simple logistic regression to the data and look at how well it makes predictions on the data. The code below will fit a simple logistic regression , using only a single predictor (bmi), to allow for simple plots of the fit, and then using all available features. \n",
        "\n",
        "We begin by plotting the data, the model fit and some classic summary statistics. \n",
        "\n",
        "*Questions*:\n",
        " - Which features are the most predictive?\n",
        " - How much improvement to you get if you combined features compared to a model based on individual features?\n",
        " - Try regenerating the dataset with more or less noise. What happens to prediction accuracy as you add more noisy variables?\n",
        "\n",
        " Note: the data has a bunch of variables called noise_<number> e.g noise_1, noise_2 etc. These are just randomly generated numbers. But in real life these types of variables do exist - they are essentially any variable that is unrelated to the thing we are trying to predict. In this data, we explicitly know these noise variables but in your own datasets (and for the other variables in the diabetes dataset), we have no idea which variables are related and which are not. \n",
        " \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "2F0ps1rO9SlX",
        "outputId": "9560297c-0f66-4453-ace9-8f4094e789ab",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "\n",
        "# If you feel like generating a new dataset, uncomment this line. \n",
        "#X, y, X_ext, y_ext = load_diabetes_data(n_noise=20)\n",
        "\n",
        "# The features used to construct the model\n",
        "# Change these and see how the results change\n",
        "# This option would include all features\n",
        "features= X.columns\n",
        "\n",
        "# We can also look at individual features\n",
        "#features=\"bmi\"\n",
        "#features=\"age\"\n",
        "#features=\"noise_1\"\n",
        "\n",
        "# This set will include 4 features\n",
        "#features=[\"age\", \"sex\", \"bmi\", \"glu\"]\n",
        "\n",
        "\n",
        "# sklearn models expect a 2D array. However, when only a single column is selected in pandas, it is a\n",
        "# 1D array. This conditional checks for when our input is a single column and turns it into a 2D array for sklearn.  \n",
        "if type(features)==str:\n",
        "    X_train    = X.loc[:, features].values.reshape(-1,1)\n",
        "    X_eval = X_ext.loc[:, features].values.reshape(-1,1)\n",
        "else:\n",
        "    X_train    = X.loc[:, features]\n",
        "    X_eval = X_ext.loc[:, features]\n",
        "        \n",
        "#Construct a classifier\n",
        "clf = LogisticRegression(penalty=None, solver=\"saga\", tol=0.01).fit(X_train, y)\n",
        "\n",
        "# Get predicted labels from the classifier. Here, the 'predict_proba' function returns probabilities of the labels (e.g 75% of belonging to class 1)\n",
        "# get predictions when given the training data\n",
        "y_pred = clf.predict_proba(X_train)[:,1]\n",
        "# get predictions when given the evaluation data\n",
        "y_ext_pred = clf.predict_proba(X_eval)[:,1]\n",
        "\n",
        "\n",
        "fig, ax=plt.subplots(nrows=1,ncols=2, figsize=(10,5), dpi= 100, facecolor='w', edgecolor='k')\n",
        "\n",
        "# PLot a ROC curve and show the area under the curve. \n",
        "plot_roc(y, y_pred, ax=ax[0], label=\"Training\")\n",
        "plot_roc(y_ext, y_ext_pred, ax=ax[1], label=\"External\")\n",
        "ax[0].legend(loc=0)\n",
        "ax[0].title.set_text('Internal')\n",
        "ax[1].legend(loc=0)\n",
        "ax[1].title.set_text('External')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFu4xZtE9SlY"
      },
      "source": [
        "## 1.4 Examine top features\n",
        "Logistic regression provides an interpretable model. To see which features are most important, we can look at the coefficients of each variable. Larger absolute values implies more impact in the predictions. \n",
        "\n",
        "\n",
        "**Examine the following:**\n",
        "\n",
        "1. Do any noise variables make it into the top 10?\n",
        "2. What if you generate lots of noise variables (>1000)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "id": "DHG7UPQM9SlZ",
        "outputId": "ab1a3d5d-402a-4a09-d41c-cc9dfb58f396"
      },
      "outputs": [],
      "source": [
        "coefs_df = pd.DataFrame.from_dict({'feature':features, 'coef':clf.coef_[0]})\n",
        "coefs_df.sort_values(by=\"coef\", key=np.abs, ascending=False).iloc[0:15, :]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<hr style=\"border:2px solid gray\"> </hr>\n",
        "\n",
        "# Session 2: models and metrics in sklearn\n",
        "\n",
        "In the previous example, we fit a logistic regression model to the given dataset and examine its performance using AUC. However, in many studies where we are looking to create a predictive model, we will be interested in creating multiple models based on different underlying algorithms and possibly evaluating them based on different criteria. Here, we demonstrate how these different models and metrics can be called and provide a few examples with our diabetes dataset as to the information you get.      "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.1 Exploring different models\n",
        "The sklearn has quite a standardised interface for fitting and applying different models. In particular clf.fit(X, y) and clf.predict_proba(X) can be used to fit a model and then extract the probabilities of the predicted classes. Comparable functions exist when looking at continuous or multi-label outcomes. \n",
        "\n",
        "These standardised interfaces allow us to easily explore the impact of different classifiers for a problem. UNderstanding the different assumptions and methods is beyond this workshop. However, we can explore how this is done and talk through the impact of diffent classifiers. \n",
        "\n",
        "\n",
        "**Examine the following:**\n",
        "\n",
        "1. How well do different classifiers perform on the given dataset? Which models maximise performance on the training set? What does external performance look like?\n",
        "2. Are there noticable timing differences?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "#X, y, X_ext, y_ext = load_diabetes_data(n_noise=20)\n",
        "\n",
        "\n",
        "#\n",
        "# Change these options to change your classifier!\n",
        "#\n",
        "clf=LogisticRegression(penalty=None, solver=\"saga\", tol=0.01)\n",
        "#clf=LogisticRegression(penalty='l2',C=10, tol=0.01)\n",
        "#clf=RandomForestClassifier(max_depth=1, random_state=0)\n",
        "#clf=DecisionTreeClassifier(max_depth=None, random_state=0)\n",
        "\n",
        "#Fit the classifier of interest\n",
        "clf.fit(X_train, y)\n",
        "\n",
        "# Get predicted labels from the classifier. Here, the 'predict_proba' function returns probabilities of the labels (e.g 75% of belonging to class 1)\n",
        "y_pred = clf.predict_proba(X_train)[:,1]\n",
        "y_ext_pred = clf.predict_proba(X_eval)[:,1]\n",
        "\n",
        "fig, ax=plt.subplots(nrows=1,ncols=2, figsize=(10,5), dpi= 100, facecolor='w', edgecolor='k')\n",
        "\n",
        "# PLot a ROC curve and show the area under the curve. \n",
        "plot_roc(y, y_pred, ax=ax[0], label=\"Training\")\n",
        "plot_roc(y_ext, y_ext_pred, ax=ax[1], label=\"External\")\n",
        "ax[0].legend(loc=0)\n",
        "ax[0].title.set_text('Internal')\n",
        "ax[1].legend(loc=0)\n",
        "ax[1].title.set_text('External')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.2 Example of a specific model output\n",
        "Different machine learning methods have very different underlying algorithms and can produce very different outputs. \n",
        "\n",
        "Below we take an example at a unique output, that of a decision tree, to peek into how it works. Here, the model produces a sort of flowchart for how a sample should be classifier, based on a series of binary decisions. We visualise this model below where each node shows\n",
        "1. A variable chosen to make a decision and a threshold for which side of the subtree we go down\n",
        "2. gini value, a measure of how well the given split separates classes (lower is more discriminative)\n",
        "3. samples is the number of samples in a node in the training data\n",
        "4. values is the number of samples in each class in the training data (sum of 'values' is the same as 'samples')\n",
        "\n",
        "**Examine the following:**\n",
        "1. What happens when we have more or less noise variables in the data? Do they make it into the tree?\n",
        "2. What if we make the tree bigger or smaller (n=1 or n=5)? How do you think the model will perform\n",
        "3. Try make the max_depth=7. Is the tree still interpretable? How many noise variables are included"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn import tree\n",
        "\n",
        "#Fit a decision tree. \n",
        "#   Max depth controls how many rules/layers your tree has. \n",
        "#   Random state is so that the random part of the tree is replicable\n",
        "clf=DecisionTreeClassifier(max_depth=3, random_state=0)\n",
        "clf.fit(X_train, y)\n",
        "\n",
        "#\n",
        "# These parameters control the size of the resulting figure. If your model is too deep, you may need to make these values larger\n",
        "plt.figure(figsize=(30,20))  \n",
        "tree.plot_tree(clf, feature_names = X_train.columns, fontsize=12)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.3 Exploring different metrics\n",
        "\n",
        "We can also change the way that we evaluate model performance, again through the standardised interface provided by scikit-learn\n",
        "\n",
        "A list of possible options are provided can be see at\n",
        "https://scikit-learn.org/0.16/modules/model_evaluation.html\n",
        "\n",
        "Examine the following:\n",
        "\n",
        "Do different metrics ever change the ranking of which methods are best?\n",
        "How do results compare on the internal and external predictions? Are they the same? Do they dramatically differ?\n",
        "What happens when you adjust the amount of noise variables in the dataset?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# If you feel like generating a new dataset, uncomment this line. \n",
        "# X, y, X_ext, y_ext = load_diabetes_data(n_noise=100)\n",
        "\n",
        "# Create a dictionary of multiple classifiers\n",
        "# Use the scikit learn website to try and learn more about the parameters\n",
        "#  https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
        "#  https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
        "clfs = {\n",
        "    'Ridge':LogisticRegression(penalty='l2',C=10, tol=0.01), \n",
        "    'LogisticReg':LogisticRegression(penalty=None, solver=\"saga\", tol=0.01), \n",
        "    'Lasso':LogisticRegression(penalty='l1', C=100, solver=\"saga\", tol=0.01), \n",
        "    'RandomForest':RandomForestClassifier(max_depth=20, random_state=0), \n",
        "    'DecisionTree':DecisionTreeClassifier(max_depth=3, random_state=0)\n",
        "}\n",
        "\n",
        "#Different metrics we might be interested in\n",
        "metrics_dict = {\n",
        "    'Accuracy':metrics.accuracy_score,\n",
        "    'BalAccuracy':metrics.balanced_accuracy_score,\n",
        "    'AUC':metrics.roc_auc_score,\n",
        "    'LogLoss':metrics.log_loss,\n",
        "    'PPV':metrics.precision_score,\n",
        "    'NPV':metrics.precision_score,\n",
        "}\n",
        "\n",
        "#Helper function to return classifier name\n",
        "def get_clf_name(estimator):\n",
        "    return(estimator.__class__.__name__)\n",
        "\n",
        "scores = {}\n",
        "for clf_name,clf in clfs.items():\n",
        "    scores[clf_name] = {}\n",
        "    clf.fit(X, y)\n",
        "    for metric_name, metric in metrics_dict.items():\n",
        "        scores[clf_name][metric_name + '_Internal']=metric(y, clf.predict(X))\n",
        "        scores[clf_name][metric_name + '_External']=metric(y_ext, clf.predict(X_ext))\n",
        "\n",
        "pd.options.display.float_format = '{:,.2f}'.format\n",
        "pd.DataFrame(scores)\n",
        "\n",
        "\n",
        "#clfs_res=[]\n",
        "#print(\"{} total classifiers: \".format(len(clfs.items())), end=\"\")\n",
        "#for i, (clf_name, clf) in enumerate(clfs.items()):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tdicVuS9SlZ",
        "scrolled": true
      },
      "source": [
        "<hr style=\"border:2px solid gray\"> </hr>\n",
        "\n",
        "# Session 3: Train/test and cross-validation frameworks\n",
        "\n",
        "In the previous example, we built model on entire dataset and evaluated its performance on the same data. Here, we will explore some alternative frameworks for doing this and will evaluate how model performance changes. We'll also start to explore different models and how key parameters can be altered to change prediction performance. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4oWEBEBk9Sla"
      },
      "source": [
        "## 3.1 Training and Test split\n",
        "\n",
        "The following code examines training and testing a model on a single dataset and compares its performance to an external dataset in 3 scenarios:\n",
        " - Training on the entire dataset. Test on the same entire dataset. \n",
        " - Training on a proportion (default 80%). Test on the same proportion. \n",
        " - Training on a proportion (default 80%). Test on the remaining proportion. \n",
        " \n",
        "**Questions:**\n",
        "1. Run this cell a few times. Which accuracies change? Why?\n",
        "2. Which evaluation scenario is closest to the external data performance?\n",
        "2. What is the best performance you can get by default? What is the worst performance you observe?\n",
        "3. What happens to test performance as you add more noise variables?\n",
        "4. Try changing parameters ('C' for the l2 penalized logistic regression, or max_depth for Random Forest)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "CpZ7Q5jx9Sla",
        "outputId": "c75ec7fd-1b4b-477a-f63a-374f21de105e"
      },
      "outputs": [],
      "source": [
        "# If you feel like generating a new dataset, uncomment this line. \n",
        "#X, y, X_ext, y_ext = load_diabetes_data(n_noise=40)\n",
        "\n",
        "#\n",
        "# Select a model to construct\n",
        "#\n",
        "#clf=LogisticRegression(penalty=None, solver=\"saga\", tol=0.01)\n",
        "#clf=LogisticRegression(penalty='l2',C=10, tol=0.01)\n",
        "clf=RandomForestClassifier(max_depth=10, random_state=0)\n",
        "\n",
        "# Propotion of data to use for testing\n",
        "test_prop = 0.2\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_prop)\n",
        "\n",
        "scenarios = {\n",
        "                \"Tr:all_Te:all\":    {\"train\":{'X':X,'y':y}, \"test\":{'X':X,'y':y}}, \n",
        "                \"Tr:train_Te:train\":  {\"train\":{'X':X_train,'y':y_train}, \"test\":{'X':X_train,'y':y_train}}, \n",
        "                \"Tr:train_Te:test\":   {\"train\":{'X':X_train,'y':y_train}, \"test\":{'X':X_test,'y':y_test}}\n",
        "            }\n",
        "\n",
        "fig, ax=plt.subplots(nrows=1,ncols=2, figsize=(10,5), dpi= 100, facecolor='w', edgecolor='k')\n",
        "\n",
        "\n",
        "# Fit the model on the internal data, make predictions on whatever we are calling the test data and plot the results\n",
        "for name, sc in scenarios.items():\n",
        "    clf.fit(sc['train']['X'], sc['train']['y'])\n",
        "    \n",
        "    yp=clf.predict_proba(sc['test']['X'])[:,1]    \n",
        "    plot_roc(sc['test']['y'], yp, name, ax=ax[0])\n",
        "   \n",
        "# Fit the model on the external data and plot the results\n",
        "clf.fit(X, y)\n",
        "yp_ext=clf.predict_proba(X_ext)[:,1]\n",
        "plot_roc(y_ext, yp_ext, \"External\", ax=ax[1])\n",
        "    \n",
        "ax[0].legend(loc=0)\n",
        "ax[0].title.set_text('Internal')\n",
        "ax[1].legend(loc=0)\n",
        "ax[1].title.set_text('External')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJu0PpDB9Slb"
      },
      "source": [
        "## 3.2 K-fold Cross-validation\n",
        "Variability in the performance of different splits in the previous example motivates the use of K-fold cross validation. Here, we explore a few models and start to compare model performance.  \n",
        "\n",
        "***Warning***: Be careful setting the values below. Setting the number of times to evaluate the classifiers too high and it will take too long to run for this workshop. \n",
        "\n",
        "**Questions:**\n",
        "1. Run this cell a few times. What is the range of the scores that are observed?\n",
        "2. Which model is the best? How do you determine this?\n",
        "3. Play around with hyperparameters, what is the impact on model performance? Which models are sensitive to these choices?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 474
        },
        "id": "C11BomI69Slc",
        "outputId": "95bd92cf-a0c1-4dea-a9f3-e2823c6afb78"
      },
      "outputs": [],
      "source": [
        "# If you feel like generating a new dataset, uncomment this line. \n",
        "# X, y, X_ext, y_ext = load_diabetes_data(n_noise=100)\n",
        "\n",
        "clfs = {\n",
        "    'ridge':LogisticRegression(penalty='l2',C=10, tol=0.01), \n",
        "    'logreg':LogisticRegression(penalty=None, solver=\"saga\", tol=0.01), \n",
        "    'lasso':LogisticRegression(penalty='l1', C=100, solver=\"saga\", tol=0.01), \n",
        "    'RandomForest':RandomForestClassifier(max_depth=20, random_state=0), \n",
        "}\n",
        "\n",
        "def get_clf_name(estimator):\n",
        "    return(estimator.__class__.__name__)\n",
        "\n",
        "# This constructs n_splits * n_repeats classifiers. If these values are large, \n",
        "# or if classifier is slow it may take a long time    \n",
        "n_folds=5\n",
        "n_reps=3\n",
        "cv = RepeatedKFold(n_splits=n_folds,n_repeats=n_reps)\n",
        "\n",
        "clfs_res=[]\n",
        "print(\"{} total classifiers: \".format(len(clfs.items())), end=\"\")\n",
        "for i, (clf_name, clf) in enumerate(clfs.items()):\n",
        "    print(i, end=\"\")\n",
        "    \n",
        "    # Record the  AUC for this classifier\n",
        "    scores = cross_val_score(clf, X, y, cv=cv, scoring=\"roc_auc\")\n",
        "    \n",
        "    #Turn the results into a data frame and add the classifier name\n",
        "    clf_res=pd.DataFrame(data = scores, columns = ['auc']).assign(clf=clf_name)\n",
        "    clfs_res.append(clf_res)\n",
        "print(\"Done\", end=\"\")\n",
        "\n",
        "#Make one big dataframe rather than a list of data.frames\n",
        "clfs_res_merge = pd.concat(clfs_res)\n",
        "\n",
        "#Now make a boxplot of the AUCs\n",
        "fig, ax=plt.subplots(nrows=1,ncols=1, figsize=(10,5), dpi= 100, facecolor='w', edgecolor='k')\n",
        "sns.boxplot(data=clfs_res_merge, y='auc', x=\"clf\",ax=ax )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7UftUfo9Sld"
      },
      "source": [
        "# Session 4. Overfitting via feature selection and model parameters\n",
        "\n",
        "\n",
        "## 4.1 Feature discrimination over entire dataset\n",
        "One naive way to remove noise is to look at the features one-by-one,  look at their ability to discriminate the dataset and only take the most useful into our model. This is flawed but is common in the literature. \n",
        "\n",
        "Lets take a look at the discriminatory ability of our features, here using an ANOVA, a common statistical test. We report the f-statistic (a measure of effect size) and p-value for each features. \n",
        "\n",
        "**Question:** \n",
        "1. How do measured and noise features compare?\n",
        "2. What if we generate lots (n=10,000) noise features? How often can we distinguish noise and real signal?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "WpTcAPfM9Sld",
        "outputId": "e3edf5cb-f4f6-4f31-ae63-720f00dfc129"
      },
      "outputs": [],
      "source": [
        "# If you feel like generating a new dataset, uncomment this line. \n",
        "# X, y, X_ext, y_ext = load_diabetes_data(n_noise=10)\n",
        "\n",
        "# Look at which features are important over the entire dataset\n",
        "f, p = feature_selection.f_classif(X, y)\n",
        "feature_scores = pd.DataFrame.from_dict({\"feature\":X.columns, \"f-stat\":f, \"p.val\":p})\n",
        "feature_scores.sort_values(by=\"p.val\", key=np.abs, ascending=True).iloc[0:10, :]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldM_WpdT9Sle"
      },
      "source": [
        "## 4.2 Demonstrating potential overfitting when selecting features before CV\n",
        "Given we've ranked the features in terms of their discrinatory ability, we could now select some top amount (based on p-value, f-statistic or a feeling for how many features we need). \n",
        "\n",
        "But such an approach uses all of the information, and hence means there is no unsed data left for an untouched test set. \n",
        "\n",
        "To explore this impact, the code below plots classifier performance starting with a single most discrinimatory feature and increasing to the top 32 features. We plot the model performance in training and testing. \n",
        "\n",
        "Additionally, we plot the performance of constructing a model on all samples and evaluating the external dataset to show where the ideal would be. \n",
        "\n",
        "**Questions**\n",
        "1. What are the trends in the performance of the model on the training data as we increase features?\n",
        "2. What are the trends in the performance of the model on the test data as we increase features?\n",
        "3. Where is the ideal number of features for the external data?\n",
        "4. How often does number of features to achieve the highest \"test\" performance correspond to the hihgest external performance?\n",
        "5. What happens if you run this cell a few times? How do results differ? Why do they change?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "1bzTAixB9Sle",
        "outputId": "b1d0f49e-05c8-414f-8e53-7db7720420ab"
      },
      "outputs": [],
      "source": [
        "#\n",
        "# Pick your classifier\n",
        "#\n",
        "clf = LogisticRegression(penalty=None, tol=0.01, solver='saga')\n",
        "#clf = RandomForestClassifier(max_depth=20, random_state=0)\n",
        "\n",
        "\n",
        "# Set up a train/test split. You can change 'test_prop' to any (0-1) value\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_prop)\n",
        "\n",
        "#Look at different numbers of features in the model. Remove any that are bigger than our current dataframe\n",
        "n_feats = [1,2,3, 4,5, 6, 8,12,16,20, 24, 28, 32, 64, 128, 1000]\n",
        "n_feats = filter(lambda x: x<=len(X_train.columns), n_feats)\n",
        "aucs=[]\n",
        "for i in n_feats:\n",
        "\n",
        "    #This is a clever trick, where we make a pikeline that first searches \n",
        "    # for the 'k' best features and then applies a classifier  to the filtered data. \n",
        "    #\n",
        "    # Here, the best is based on an ANOVA F-statistic \n",
        "    # https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_classif.html\n",
        "    # \n",
        "    # But you could use a range of other functions\n",
        "    # https://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection\n",
        "    clf_selected = make_pipeline(SelectKBest(f_classif, k=i),clf)\n",
        "\n",
        "    #\n",
        "    #\n",
        "    ## Two possibilities to choose from\n",
        "    #   Biased: Use this to conduct feature selection on the entire dataset\n",
        "    clf_selected.fit(X, y)\n",
        "    #   Unbiased: Use this to conduct feature selection only on the training data\n",
        "    #clf_selected.fit(X_train, y_train)\n",
        "      \n",
        "    aucs.append(pd.DataFrame.from_dict({\n",
        "        \"n_feat\": [i,i],\n",
        "        \"model\" : [\"Train\", \"Test\"],\n",
        "        \"auc\" : [clf_selected.score(X_train, y_train), \n",
        "                   clf_selected.score(X_test, y_test)]\n",
        "    }))\n",
        "    \n",
        "    clf_selected.fit(X, y)\n",
        "    aucs.append(pd.DataFrame.from_dict({\n",
        "        \"n_feat\": [i],\n",
        "        \"model\" : [\"External\"],\n",
        "        \"auc\" : [clf_selected.score(X_ext, y_ext)]\n",
        "    }))\n",
        "    \n",
        "    \n",
        "    \n",
        "aucs_df=pd.concat(aucs, ignore_index=True)\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "f=sns.lineplot(data=aucs_df, x=\"n_feat\", y=\"auc\", hue=\"model\")\n",
        "f.set(xscale='log')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9LgLCyUG9Slf"
      },
      "source": [
        "## 4.3 Nested cross-validation for feature selection and hyperparameter tuning\n",
        "\n",
        "Rather than selecting the number of features to be included in a model (based on a random guess, or worse peeking at test results), we can instead conduct feature selection as part of cross validation. \n",
        "\n",
        "The code below runs two cross-validation loops (inner and outer), essentially running one loop on the training data (repeatededly breaking it into training and validation datasets) to understand how the number of features impacts performance. We then select the best number of features and evaluate the held-out testset. This is then repeated for the number of folds in the outer loop. \n",
        "\n",
        "While robust, the approach can be computationally expensive as we are building many models. \n",
        "\n",
        "**Warning** This code will take a few minutes. If you add in hyperparameter selection (by uncommenting param_grid),  this could take quite a while to run in Google Colab.\n",
        "\n",
        "**Questions:**\n",
        "1. Try generating a dataset with no noisy features and one with many? How much does performance vary?\n",
        "2. How does performance vary if we change from a penalized regression to a random forest?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327
        },
        "id": "EGKR08T_9Slf",
        "outputId": "2c0722f5-62ea-4e2c-f1f6-808cc8c15f16"
      },
      "outputs": [],
      "source": [
        "# Form a grid of parameters to look over\n",
        "n_feats=[1,2,3, 4,5, 6, 8,12,16,20, 24, 28, 32]\n",
        "n_feats=list(filter(lambda x: x<=len(X_train.columns), n_feats))\n",
        "\n",
        "# Fit a regularised logistic regression\n",
        "clf = LogisticRegression(penalty='l2', solver=\"saga\", tol=0.01)\n",
        "# Fit a random forest\n",
        "#clf = RandomForestClassifier(max_depth=20, random_state=0)\n",
        "\n",
        "\n",
        "#This line only optimises # of features selected. Next two also optimise model parameters but I think this is too slow for colab. \n",
        "param_grid = {\"selectkbest__k\": n_feats}\n",
        "#param_grid = {\"selectkbest__k\": n_feats, \"logisticregression__C\": [0.01, 0.1, 1,5, 10, 50, 100]}\n",
        "#param_grid = {\"selectkbest__k\": n_feats, \"randomforestclassifier__max_depth\": [5, 20, 50, 100]}\n",
        "\n",
        "\n",
        "model_to_tune = make_pipeline(SelectKBest(f_classif),clf)\n",
        "    \n",
        "test_score_not_nested = []\n",
        "test_score_nested = []\n",
        "\n",
        "n_rep = 3\n",
        "n_split_outer=5\n",
        "n_split_inner=3\n",
        "\n",
        "for i in range(n_rep):\n",
        "    print(i, end='')\n",
        "\n",
        "    #Set up two cross-validation helpers\n",
        "    # Could also use RepeatedKFold() here but quickly becomes slow to fit\n",
        "    inner_cv = KFold(n_splits=n_split_inner, shuffle=True, random_state=i)\n",
        "    outer_cv = KFold(n_splits=n_split_outer, shuffle=True, random_state=i)\n",
        "\n",
        "    # Warning: the code below is not very obvious\n",
        "    \n",
        "    # 1. Evaluate Non-nested parameter search and scoring\n",
        "    #   GridSearchCV will take a model to fit and a grid of parameters and will evaluate performance \n",
        "    #   as a bunch of test/train splits. We can then select the model with the best score on the test data\n",
        "    #   But this is biased as we evaluate the model based on the test data. \n",
        "    model = GridSearchCV(estimator=model_to_tune, param_grid=param_grid, cv=inner_cv, scoring=\"roc_auc\")\n",
        "    model.fit(X, y)\n",
        "    test_score_not_nested.append(model.best_score_)\n",
        "\n",
        "    # 2. Evaluate Nested CV with parameter optimization\n",
        "    #     cross_val_score() will do all of our model fitting, first splitting the data into test and the remainder of the data then gets \n",
        "    #        split using GridSearchCV() into training and validation. The best model is selected from performance on the validation data\n",
        "    #         and this final model is used to evaluate the held-out test data. \n",
        "    # Unbiased as model selection takes place on the validation, rather than test data. \n",
        "    model = GridSearchCV(estimator=model_to_tune, param_grid=param_grid, cv=inner_cv, scoring=\"roc_auc\")\n",
        "    test_score = cross_val_score(model, X, y, cv=outer_cv, scoring=\"roc_auc\")\n",
        "    test_score_nested.append(test_score.mean())\n",
        "\n",
        "\n",
        "all_scores = {\n",
        "    \"Not nested CV\": test_score_not_nested,\n",
        "    \"Nested CV\": test_score_nested,\n",
        "}\n",
        "all_scores = pd.DataFrame(all_scores)\n",
        "\n",
        "color = {\"whiskers\": \"black\", \"medians\": \"black\", \"caps\": \"black\"}\n",
        "all_scores.plot.box(color=color, vert=True)\n",
        "plt.xlabel(\"AUC\")\n",
        "plt.title(\"Comparison of mean accuracy obtained on the test sets with\\n\"\n",
        "              \"and without nested cross-validation\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFqz3wBC9Slg"
      },
      "source": [
        "## Final task\n",
        "Using the above code, can you implement a scheme to answer the question \"do blood serum markers help predict diabetes progression beyond age, sex, bmi and blood glucose?\".\n",
        "\n",
        "Steps: \n",
        " - Copy the code in the previous section\n",
        " - Add call to a model with a specified feature subset (as in section Cell 1.3)\n",
        " - Compare boxplots from these two models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_33fRxC99Slh"
      },
      "outputs": [],
      "source": [
        "# This is probably outside of the scope of this workshop! But if you get to here and have time to think about this, talk to me about your proposed solution. "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.10.7 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0e52c6cba9864d0f89c9171c038226e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1b927521ecab43bc937c7fad732e4138",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d3fc7bdc02c0491c8e6b6498984fba6a",
            "value": 1
          }
        },
        "1b927521ecab43bc937c7fad732e4138": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1e075c7bb2b341fb9d3be3948631d9c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3990a336e1d14dcb9eb37f9dfc10a710",
            "placeholder": "",
            "style": "IPY_MODEL_88e9ee2dbb29440fa181bb656fc90407",
            "value": "Render HTML: 100%"
          }
        },
        "2210402b98fb4bf2a1627cba57c386e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1e075c7bb2b341fb9d3be3948631d9c6",
              "IPY_MODEL_0e52c6cba9864d0f89c9171c038226e8",
              "IPY_MODEL_e0b6ba0328cc49c19598d544f64a6e63"
            ],
            "layout": "IPY_MODEL_748a8e9291584a6bb88da45a5977c4d8"
          }
        },
        "3990a336e1d14dcb9eb37f9dfc10a710": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "54d8f558ba794237bee5256118ba0e00": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "748a8e9291584a6bb88da45a5977c4d8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "88e9ee2dbb29440fa181bb656fc90407": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c515e2a0d3c64229839059a29ac2db46": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d3fc7bdc02c0491c8e6b6498984fba6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e0b6ba0328cc49c19598d544f64a6e63": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c515e2a0d3c64229839059a29ac2db46",
            "placeholder": "",
            "style": "IPY_MODEL_54d8f558ba794237bee5256118ba0e00",
            "value": " 1/1 [00:03&lt;00:00,  3.77s/it]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
