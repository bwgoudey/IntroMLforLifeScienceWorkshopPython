{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0eQobFj9Sk7"
      },
      "source": [
        "# Workshop -- Machine learning in life sciences\n",
        "### What is it, when should it be used and how to avoid common pitfalls\n",
        "\n",
        "**Author:** Benjamin Goudey, Research Fellow in Florey Department of Neuroscience and Mental Health at The University of Melbourne\n",
        "\n",
        "**Last updated:** 19/08/2024"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gyo-XHug9SlA"
      },
      "source": [
        "# Introduction\n",
        "Welcome to the workshop! This notebook accompanies the workshop *Applying machine learning in life sciences: what does it mean and how to avoid common traps.*\n",
        "\n",
        "The notebook focuses on the problem of predicting diabetes from a few clinical and blood measurements, as well as several simulated variables that are not correlated with the outcome.\n",
        "\n",
        "The notebook is split into into four sections.\n",
        "\n",
        "0. Welcome and setup\n",
        "1. Exploring the data and fitting a model and measuring performance\n",
        "2. Pitfall 1: Evaluation frameworks and generalisation\n",
        "3.  Pitfall 2: Selecting features and model parameters\n",
        "\n",
        "There will be a number of models, measures and algorithms that will be used and will be briefly explained in the accompanying tutorial but will not be covered in detail. The skikit-learn documentation will be valuable here (https://scikit-learn.org/stable/modules/classes.html)\n",
        "\n",
        "The notebook assumes familiarity with Python, and a passing familiarity with the pandas, matplotlib/seaborn and numpy/scipy packages. But even if you don't have this, the idea is that this notebook should help you get an idea of some of the concepts around machine learning and may be a useful resource for you at some stage.\n",
        "\n",
        "**Please note:** the expectation is you should be able to follow along rather than write this code from scratch. You should be able to run each cell in the notebook to get an output and then comments should direct you to indicate which parameters to change. If you get stuck, let us know!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odlRpgfkM282"
      },
      "source": [
        "# 0 Package Setup and Data Loading\n",
        "\n",
        "Don't worry too much about the code in this section. We load in the necessary packages and then there are a number of functions to load in the data or plot outputs. The details of these are mostly not needed for this workshop and I'll step through in more detail when they are needed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75-A4WtJM8fi"
      },
      "source": [
        "## 0.1 Install and load required packages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQBBOTyU8Obj"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "**Warning:** you may need to run this cell twice\n",
        "\n",
        "It installs a recent version of ydata-profiling, a tool for summarising the content of a pandas data frame. When the install occurs, a little reset button may appear. Press this and re-run the cell to load all packages into your environment.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "vFSeoWp_9SlC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e82a314-1bd2-4cc3-d194-0067424861b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tableone\n",
            "  Downloading tableone-0.9.1-py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: ydata-profiling[notebook] in /usr/local/lib/python3.10/dist-packages (4.9.0)\n",
            "Requirement already satisfied: scipy<1.14,>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ydata-profiling[notebook]) (1.13.1)\n",
            "Requirement already satisfied: pandas!=1.4.0,<3,>1.1 in /usr/local/lib/python3.10/dist-packages (from ydata-profiling[notebook]) (2.1.4)\n",
            "Requirement already satisfied: matplotlib<3.10,>=3.5 in /usr/local/lib/python3.10/dist-packages (from ydata-profiling[notebook]) (3.7.1)\n",
            "Requirement already satisfied: pydantic>=2 in /usr/local/lib/python3.10/dist-packages (from ydata-profiling[notebook]) (2.8.2)\n",
            "Requirement already satisfied: PyYAML<6.1,>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from ydata-profiling[notebook]) (6.0.2)\n",
            "Requirement already satisfied: jinja2<3.2,>=2.11.1 in /usr/local/lib/python3.10/dist-packages (from ydata-profiling[notebook]) (3.1.4)\n",
            "Requirement already satisfied: visions<0.7.7,>=0.7.5 in /usr/local/lib/python3.10/dist-packages (from visions[type_image_path]<0.7.7,>=0.7.5->ydata-profiling[notebook]) (0.7.6)\n",
            "Requirement already satisfied: numpy<2,>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from ydata-profiling[notebook]) (1.26.4)\n",
            "Requirement already satisfied: htmlmin==0.1.12 in /usr/local/lib/python3.10/dist-packages (from ydata-profiling[notebook]) (0.1.12)\n",
            "Requirement already satisfied: phik<0.13,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from ydata-profiling[notebook]) (0.12.4)\n",
            "Requirement already satisfied: requests<3,>=2.24.0 in /usr/local/lib/python3.10/dist-packages (from ydata-profiling[notebook]) (2.32.3)\n",
            "Requirement already satisfied: tqdm<5,>=4.48.2 in /usr/local/lib/python3.10/dist-packages (from ydata-profiling[notebook]) (4.66.5)\n",
            "Requirement already satisfied: seaborn<0.14,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from ydata-profiling[notebook]) (0.13.1)\n",
            "Requirement already satisfied: multimethod<2,>=1.4 in /usr/local/lib/python3.10/dist-packages (from ydata-profiling[notebook]) (1.4)\n",
            "Requirement already satisfied: statsmodels<1,>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from ydata-profiling[notebook]) (0.14.2)\n",
            "Requirement already satisfied: typeguard<5,>=3 in /usr/local/lib/python3.10/dist-packages (from ydata-profiling[notebook]) (4.3.0)\n",
            "Requirement already satisfied: imagehash==4.3.1 in /usr/local/lib/python3.10/dist-packages (from ydata-profiling[notebook]) (4.3.1)\n",
            "Requirement already satisfied: wordcloud>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from ydata-profiling[notebook]) (1.9.3)\n",
            "Requirement already satisfied: dacite>=1.8 in /usr/local/lib/python3.10/dist-packages (from ydata-profiling[notebook]) (1.8.1)\n",
            "Requirement already satisfied: numba<1,>=0.56.0 in /usr/local/lib/python3.10/dist-packages (from ydata-profiling[notebook]) (0.60.0)\n",
            "Requirement already satisfied: jupyter>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ydata-profiling[notebook]) (1.0.0)\n",
            "Requirement already satisfied: ipywidgets>=7.5.1 in /usr/local/lib/python3.10/dist-packages (from ydata-profiling[notebook]) (7.7.1)\n",
            "Requirement already satisfied: PyWavelets in /usr/local/lib/python3.10/dist-packages (from imagehash==4.3.1->ydata-profiling[notebook]) (1.7.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from imagehash==4.3.1->ydata-profiling[notebook]) (9.4.0)\n",
            "Requirement already satisfied: openpyxl>=3.1.2 in /usr/local/lib/python3.10/dist-packages (from tableone) (3.1.5)\n",
            "Requirement already satisfied: tabulate>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from tableone) (0.9.0)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=7.5.1->ydata-profiling[notebook]) (5.5.6)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=7.5.1->ydata-profiling[notebook]) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=7.5.1->ydata-profiling[notebook]) (5.7.1)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=7.5.1->ydata-profiling[notebook]) (3.6.8)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=7.5.1->ydata-profiling[notebook]) (7.34.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=7.5.1->ydata-profiling[notebook]) (3.0.11)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2<3.2,>=2.11.1->ydata-profiling[notebook]) (2.1.5)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.10/dist-packages (from jupyter>=1.0.0->ydata-profiling[notebook]) (6.5.5)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.10/dist-packages (from jupyter>=1.0.0->ydata-profiling[notebook]) (5.5.2)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.10/dist-packages (from jupyter>=1.0.0->ydata-profiling[notebook]) (6.1.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.10/dist-packages (from jupyter>=1.0.0->ydata-profiling[notebook]) (6.5.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<3.10,>=3.5->ydata-profiling[notebook]) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib<3.10,>=3.5->ydata-profiling[notebook]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<3.10,>=3.5->ydata-profiling[notebook]) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<3.10,>=3.5->ydata-profiling[notebook]) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<3.10,>=3.5->ydata-profiling[notebook]) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<3.10,>=3.5->ydata-profiling[notebook]) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib<3.10,>=3.5->ydata-profiling[notebook]) (2.8.2)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba<1,>=0.56.0->ydata-profiling[notebook]) (0.43.0)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl>=3.1.2->tableone) (1.1.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas!=1.4.0,<3,>1.1->ydata-profiling[notebook]) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas!=1.4.0,<3,>1.1->ydata-profiling[notebook]) (2024.1)\n",
            "Requirement already satisfied: joblib>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from phik<0.13,>=0.11.1->ydata-profiling[notebook]) (1.4.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2->ydata-profiling[notebook]) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2->ydata-profiling[notebook]) (2.20.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2->ydata-profiling[notebook]) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.24.0->ydata-profiling[notebook]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.24.0->ydata-profiling[notebook]) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.24.0->ydata-profiling[notebook]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.24.0->ydata-profiling[notebook]) (2024.7.4)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.10/dist-packages (from statsmodels<1,>=0.13.2->ydata-profiling[notebook]) (0.5.6)\n",
            "Requirement already satisfied: attrs>=19.3.0 in /usr/local/lib/python3.10/dist-packages (from visions<0.7.7,>=0.7.5->visions[type_image_path]<0.7.7,>=0.7.5->ydata-profiling[notebook]) (24.2.0)\n",
            "Requirement already satisfied: networkx>=2.4 in /usr/local/lib/python3.10/dist-packages (from visions<0.7.7,>=0.7.5->visions[type_image_path]<0.7.7,>=0.7.5->ydata-profiling[notebook]) (3.3)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.5.1->ydata-profiling[notebook]) (6.1.12)\n",
            "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.5.1->ydata-profiling[notebook]) (6.3.3)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5.1->ydata-profiling[notebook]) (71.0.4)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5.1->ydata-profiling[notebook]) (0.19.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5.1->ydata-profiling[notebook]) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5.1->ydata-profiling[notebook]) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5.1->ydata-profiling[notebook]) (3.0.47)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5.1->ydata-profiling[notebook]) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5.1->ydata-profiling[notebook]) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5.1->ydata-profiling[notebook]) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5.1->ydata-profiling[notebook]) (4.9.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy>=0.5.6->statsmodels<1,>=0.13.2->ydata-profiling[notebook]) (1.16.0)\n",
            "Requirement already satisfied: pyzmq<25,>=17 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter>=1.0.0->ydata-profiling[notebook]) (24.0.1)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter>=1.0.0->ydata-profiling[notebook]) (23.1.0)\n",
            "Requirement already satisfied: jupyter-core>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter>=1.0.0->ydata-profiling[notebook]) (5.7.2)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter>=1.0.0->ydata-profiling[notebook]) (5.10.4)\n",
            "Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter>=1.0.0->ydata-profiling[notebook]) (1.6.0)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter>=1.0.0->ydata-profiling[notebook]) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter>=1.0.0->ydata-profiling[notebook]) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter>=1.0.0->ydata-profiling[notebook]) (0.20.0)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.10/dist-packages (from notebook->jupyter>=1.0.0->ydata-profiling[notebook]) (1.1.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter>=1.0.0->ydata-profiling[notebook]) (4.9.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter>=1.0.0->ydata-profiling[notebook]) (4.12.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter>=1.0.0->ydata-profiling[notebook]) (6.1.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter>=1.0.0->ydata-profiling[notebook]) (0.7.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter>=1.0.0->ydata-profiling[notebook]) (0.4)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter>=1.0.0->ydata-profiling[notebook]) (0.3.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter>=1.0.0->ydata-profiling[notebook]) (0.8.4)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter>=1.0.0->ydata-profiling[notebook]) (0.10.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter>=1.0.0->ydata-profiling[notebook]) (1.5.1)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert->jupyter>=1.0.0->ydata-profiling[notebook]) (1.3.0)\n",
            "Requirement already satisfied: qtpy>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from qtconsole->jupyter>=1.0.0->ydata-profiling[notebook]) (2.4.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets>=7.5.1->ydata-profiling[notebook]) (0.8.4)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.6.1->notebook->jupyter>=1.0.0->ydata-profiling[notebook]) (4.2.2)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook->jupyter>=1.0.0->ydata-profiling[notebook]) (0.2.4)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.10/dist-packages (from nbformat->notebook->jupyter>=1.0.0->ydata-profiling[notebook]) (2.20.0)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat->notebook->jupyter>=1.0.0->ydata-profiling[notebook]) (4.23.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets>=7.5.1->ydata-profiling[notebook]) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets>=7.5.1->ydata-profiling[notebook]) (0.2.13)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi->notebook->jupyter>=1.0.0->ydata-profiling[notebook]) (21.2.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert->jupyter>=1.0.0->ydata-profiling[notebook]) (2.5)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->nbconvert->jupyter>=1.0.0->ydata-profiling[notebook]) (0.5.1)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook->jupyter>=1.0.0->ydata-profiling[notebook]) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook->jupyter>=1.0.0->ydata-profiling[notebook]) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook->jupyter>=1.0.0->ydata-profiling[notebook]) (0.20.0)\n",
            "Requirement already satisfied: jupyter-server<3,>=1.8 in /usr/local/lib/python3.10/dist-packages (from notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->jupyter>=1.0.0->ydata-profiling[notebook]) (1.24.0)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook->jupyter>=1.0.0->ydata-profiling[notebook]) (1.17.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook->jupyter>=1.0.0->ydata-profiling[notebook]) (2.22)\n",
            "Requirement already satisfied: anyio<4,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->jupyter>=1.0.0->ydata-profiling[notebook]) (3.7.1)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->jupyter>=1.0.0->ydata-profiling[notebook]) (1.8.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->jupyter>=1.0.0->ydata-profiling[notebook]) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook->jupyter>=1.0.0->ydata-profiling[notebook]) (1.2.2)\n",
            "Downloading tableone-0.9.1-py3-none-any.whl (41 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.6/41.6 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tableone\n",
            "Successfully installed tableone-0.9.1\n"
          ]
        }
      ],
      "source": [
        "%pip install ydata-profiling[notebook] tableone\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "# Load in everything we need\n",
        "# If you are playing along on your local machine, you need to run\n",
        "#     pip install pandas numpy scikit-learn graphviz matplotlib seaborn ydata_profiling ipywidgets\n",
        "# to download and install the libraries first\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, RepeatedKFold, KFold, GridSearchCV\n",
        "from sklearn import datasets\n",
        "from sklearn import metrics\n",
        "from sklearn import feature_selection\n",
        "from sklearn import linear_model\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from sklearn.base import clone\n",
        "\n",
        "import graphviz\n",
        "\n",
        "# Models\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression, Lasso\n",
        "\n",
        "# Plotting\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from tableone import TableOne\n",
        "\n",
        "\n",
        "# ydata_profiling - generates an interactive report\n",
        "from ydata_profiling import ProfileReport\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QWvjl7a9SlJ"
      },
      "source": [
        "## 0.2 Loading helper function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "kKGx6iUV9SlJ"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists('workshop_fn.py'):\n",
        "  !wget https://raw.githubusercontent.com/bwgoudey/IntroMLforLifeScienceWorkshopPython/main/workshop_fn.py\n",
        "\n",
        "import workshop_fn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c45P45H8Obl"
      },
      "source": [
        "## 0.3 Data loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "MKSLEipS8Obl"
      },
      "outputs": [],
      "source": [
        "# assigning url to a variable\n",
        "url=\"https://raw.githubusercontent.com/bwgoudey/IntroMLforLifeScienceWorkshopR/main/RC_health_data_n2000.csv\"\n",
        " # passing parameter to the function\n",
        "diabetes_df_raw =pd.read_csv(url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "L6k9BpIh8Obl",
        "outputId": "d1ab4a68-d273-4463-c164-fb7dde2aca4b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          id  age_y  gender_1_male_2_female  site  height_cm  weight_kg  \\\n",
              "0      11550     60                       2     5      160.0       64.0   \n",
              "1       8254     81                       1     5      169.0       66.0   \n",
              "2     646385     91                       1     5      163.0       59.0   \n",
              "3       7138     58                       2     5      170.0       66.0   \n",
              "4      25112     73                       1     5      161.0       64.0   \n",
              "...      ...    ...                     ...   ...        ...        ...   \n",
              "3261  681772     64                       2     5      152.0       78.0   \n",
              "3262  682796     61                       1     5      169.0       80.0   \n",
              "3263  683831     57                       1     5      173.0       66.0   \n",
              "3264  683911     69                       1     5      176.0       84.0   \n",
              "3265  684790     50                       1     8      177.0       74.0   \n",
              "\n",
              "      bmi_kg_m2  sbp_mm_hg  dbp_mm_hg  fpg_mmol_l  cholesterol_mmol_l  \\\n",
              "0          25.0        119         75        4.77                5.30   \n",
              "1          23.1        141         60        5.25                4.90   \n",
              "2          22.2        123         63        6.20                5.50   \n",
              "3          22.8        133         77        5.54                5.60   \n",
              "4          24.7        125         73        4.03                5.80   \n",
              "...         ...        ...        ...         ...                 ...   \n",
              "3261       33.8        160         80        6.60                5.10   \n",
              "3262       28.0        129         86        6.79                4.90   \n",
              "3263       22.1        134         94        6.48                5.00   \n",
              "3264       27.1        133         73        6.99                3.60   \n",
              "3265       23.6        131         85        5.40                4.96   \n",
              "\n",
              "      triglyceride_mmol_l  hdl_c_mmol_l  ldl_mmol_l  alt_u_l  bun_mmol_l  \\\n",
              "0                    0.50          1.31        3.29     19.0        7.04   \n",
              "1                    0.60          1.24        2.94     21.0        6.19   \n",
              "2                    1.10          1.50        3.31     12.0        7.22   \n",
              "3                    3.60          1.46        3.20     26.0        3.26   \n",
              "4                    1.30          1.62        3.06      9.0        5.07   \n",
              "...                   ...           ...         ...      ...         ...   \n",
              "3261                 1.10          1.59        2.93     48.0        4.42   \n",
              "3262                 1.30          1.41        2.75     19.0        7.54   \n",
              "3263                 1.60          1.26        2.98     34.0        4.87   \n",
              "3264                 1.10          1.42        1.83     43.0        6.30   \n",
              "3265                 1.71          1.16        2.82     19.0        7.30   \n",
              "\n",
              "      ccr_umol_l  year_of_followup  diabetes  \n",
              "0           68.0          3.756331         0  \n",
              "1           91.0          3.775496         0  \n",
              "2           87.0          2.844627         0  \n",
              "3           62.0          2.028747         0  \n",
              "4           83.0          3.917864         0  \n",
              "...          ...               ...       ...  \n",
              "3261        46.0          3.863107         1  \n",
              "3262       104.0          2.012320         1  \n",
              "3263        92.0          3.022587         1  \n",
              "3264        91.0          3.854894         1  \n",
              "3265        74.8          2.017796         1  \n",
              "\n",
              "[3266 rows x 19 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5a81cd10-dc23-4779-8ba9-d59007900ca0\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>age_y</th>\n",
              "      <th>gender_1_male_2_female</th>\n",
              "      <th>site</th>\n",
              "      <th>height_cm</th>\n",
              "      <th>weight_kg</th>\n",
              "      <th>bmi_kg_m2</th>\n",
              "      <th>sbp_mm_hg</th>\n",
              "      <th>dbp_mm_hg</th>\n",
              "      <th>fpg_mmol_l</th>\n",
              "      <th>cholesterol_mmol_l</th>\n",
              "      <th>triglyceride_mmol_l</th>\n",
              "      <th>hdl_c_mmol_l</th>\n",
              "      <th>ldl_mmol_l</th>\n",
              "      <th>alt_u_l</th>\n",
              "      <th>bun_mmol_l</th>\n",
              "      <th>ccr_umol_l</th>\n",
              "      <th>year_of_followup</th>\n",
              "      <th>diabetes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>11550</td>\n",
              "      <td>60</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>160.0</td>\n",
              "      <td>64.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>119</td>\n",
              "      <td>75</td>\n",
              "      <td>4.77</td>\n",
              "      <td>5.30</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.31</td>\n",
              "      <td>3.29</td>\n",
              "      <td>19.0</td>\n",
              "      <td>7.04</td>\n",
              "      <td>68.0</td>\n",
              "      <td>3.756331</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>8254</td>\n",
              "      <td>81</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>169.0</td>\n",
              "      <td>66.0</td>\n",
              "      <td>23.1</td>\n",
              "      <td>141</td>\n",
              "      <td>60</td>\n",
              "      <td>5.25</td>\n",
              "      <td>4.90</td>\n",
              "      <td>0.60</td>\n",
              "      <td>1.24</td>\n",
              "      <td>2.94</td>\n",
              "      <td>21.0</td>\n",
              "      <td>6.19</td>\n",
              "      <td>91.0</td>\n",
              "      <td>3.775496</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>646385</td>\n",
              "      <td>91</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>163.0</td>\n",
              "      <td>59.0</td>\n",
              "      <td>22.2</td>\n",
              "      <td>123</td>\n",
              "      <td>63</td>\n",
              "      <td>6.20</td>\n",
              "      <td>5.50</td>\n",
              "      <td>1.10</td>\n",
              "      <td>1.50</td>\n",
              "      <td>3.31</td>\n",
              "      <td>12.0</td>\n",
              "      <td>7.22</td>\n",
              "      <td>87.0</td>\n",
              "      <td>2.844627</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>7138</td>\n",
              "      <td>58</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>170.0</td>\n",
              "      <td>66.0</td>\n",
              "      <td>22.8</td>\n",
              "      <td>133</td>\n",
              "      <td>77</td>\n",
              "      <td>5.54</td>\n",
              "      <td>5.60</td>\n",
              "      <td>3.60</td>\n",
              "      <td>1.46</td>\n",
              "      <td>3.20</td>\n",
              "      <td>26.0</td>\n",
              "      <td>3.26</td>\n",
              "      <td>62.0</td>\n",
              "      <td>2.028747</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>25112</td>\n",
              "      <td>73</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>161.0</td>\n",
              "      <td>64.0</td>\n",
              "      <td>24.7</td>\n",
              "      <td>125</td>\n",
              "      <td>73</td>\n",
              "      <td>4.03</td>\n",
              "      <td>5.80</td>\n",
              "      <td>1.30</td>\n",
              "      <td>1.62</td>\n",
              "      <td>3.06</td>\n",
              "      <td>9.0</td>\n",
              "      <td>5.07</td>\n",
              "      <td>83.0</td>\n",
              "      <td>3.917864</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3261</th>\n",
              "      <td>681772</td>\n",
              "      <td>64</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>152.0</td>\n",
              "      <td>78.0</td>\n",
              "      <td>33.8</td>\n",
              "      <td>160</td>\n",
              "      <td>80</td>\n",
              "      <td>6.60</td>\n",
              "      <td>5.10</td>\n",
              "      <td>1.10</td>\n",
              "      <td>1.59</td>\n",
              "      <td>2.93</td>\n",
              "      <td>48.0</td>\n",
              "      <td>4.42</td>\n",
              "      <td>46.0</td>\n",
              "      <td>3.863107</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3262</th>\n",
              "      <td>682796</td>\n",
              "      <td>61</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>169.0</td>\n",
              "      <td>80.0</td>\n",
              "      <td>28.0</td>\n",
              "      <td>129</td>\n",
              "      <td>86</td>\n",
              "      <td>6.79</td>\n",
              "      <td>4.90</td>\n",
              "      <td>1.30</td>\n",
              "      <td>1.41</td>\n",
              "      <td>2.75</td>\n",
              "      <td>19.0</td>\n",
              "      <td>7.54</td>\n",
              "      <td>104.0</td>\n",
              "      <td>2.012320</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3263</th>\n",
              "      <td>683831</td>\n",
              "      <td>57</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>173.0</td>\n",
              "      <td>66.0</td>\n",
              "      <td>22.1</td>\n",
              "      <td>134</td>\n",
              "      <td>94</td>\n",
              "      <td>6.48</td>\n",
              "      <td>5.00</td>\n",
              "      <td>1.60</td>\n",
              "      <td>1.26</td>\n",
              "      <td>2.98</td>\n",
              "      <td>34.0</td>\n",
              "      <td>4.87</td>\n",
              "      <td>92.0</td>\n",
              "      <td>3.022587</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3264</th>\n",
              "      <td>683911</td>\n",
              "      <td>69</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>176.0</td>\n",
              "      <td>84.0</td>\n",
              "      <td>27.1</td>\n",
              "      <td>133</td>\n",
              "      <td>73</td>\n",
              "      <td>6.99</td>\n",
              "      <td>3.60</td>\n",
              "      <td>1.10</td>\n",
              "      <td>1.42</td>\n",
              "      <td>1.83</td>\n",
              "      <td>43.0</td>\n",
              "      <td>6.30</td>\n",
              "      <td>91.0</td>\n",
              "      <td>3.854894</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3265</th>\n",
              "      <td>684790</td>\n",
              "      <td>50</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>177.0</td>\n",
              "      <td>74.0</td>\n",
              "      <td>23.6</td>\n",
              "      <td>131</td>\n",
              "      <td>85</td>\n",
              "      <td>5.40</td>\n",
              "      <td>4.96</td>\n",
              "      <td>1.71</td>\n",
              "      <td>1.16</td>\n",
              "      <td>2.82</td>\n",
              "      <td>19.0</td>\n",
              "      <td>7.30</td>\n",
              "      <td>74.8</td>\n",
              "      <td>2.017796</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3266 rows × 19 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5a81cd10-dc23-4779-8ba9-d59007900ca0')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-5a81cd10-dc23-4779-8ba9-d59007900ca0 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-5a81cd10-dc23-4779-8ba9-d59007900ca0');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-4d53d473-b029-40cf-a6f4-fdc5757e41fe\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4d53d473-b029-40cf-a6f4-fdc5757e41fe')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-4d53d473-b029-40cf-a6f4-fdc5757e41fe button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_61215e92-d477-44a8-aefa-097feb4926ef\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('diabetes_df_raw')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_61215e92-d477-44a8-aefa-097feb4926ef button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('diabetes_df_raw');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "diabetes_df_raw",
              "summary": "{\n  \"name\": \"diabetes_df_raw\",\n  \"rows\": 3266,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 197433,\n        \"min\": 440,\n        \"max\": 684790,\n        \"num_unique_values\": 3266,\n        \"samples\": [\n          576284,\n          71401,\n          264215\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"age_y\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 13,\n        \"min\": 23,\n        \"max\": 92,\n        \"num_unique_values\": 68,\n        \"samples\": [\n          29,\n          64,\n          73\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"gender_1_male_2_female\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 2,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"site\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 5,\n        \"max\": 8,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          5,\n          8\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"height_cm\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 8.589446999963387,\n        \"min\": 122.0,\n        \"max\": 193.0,\n        \"num_unique_values\": 163,\n        \"samples\": [\n          146.5,\n          173.9\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"weight_kg\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 11.954259917777007,\n        \"min\": 34.0,\n        \"max\": 127.0,\n        \"num_unique_values\": 383,\n        \"samples\": [\n          67.3,\n          84.6\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"bmi_kg_m2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.255140502591444,\n        \"min\": 15.8,\n        \"max\": 41.5,\n        \"num_unique_values\": 200,\n        \"samples\": [\n          18.1,\n          22.3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sbp_mm_hg\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 18,\n        \"min\": 80,\n        \"max\": 204,\n        \"num_unique_values\": 114,\n        \"samples\": [\n          165,\n          125\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dbp_mm_hg\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 11,\n        \"min\": 47,\n        \"max\": 154,\n        \"num_unique_values\": 78,\n        \"samples\": [\n          64,\n          75\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"fpg_mmol_l\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.7053820547012245,\n        \"min\": 3.83,\n        \"max\": 6.99,\n        \"num_unique_values\": 310,\n        \"samples\": [\n          6.76,\n          6.06\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cholesterol_mmol_l\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.9039254768505031,\n        \"min\": 2.4,\n        \"max\": 11.9,\n        \"num_unique_values\": 388,\n        \"samples\": [\n          6.15,\n          6.79\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"triglyceride_mmol_l\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.2046822982372827,\n        \"min\": 0.18,\n        \"max\": 10.7,\n        \"num_unique_values\": 406,\n        \"samples\": [\n          2.5,\n          6.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"hdl_c_mmol_l\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.25854165293547615,\n        \"min\": 0.26,\n        \"max\": 2.57,\n        \"num_unique_values\": 159,\n        \"samples\": [\n          1.01,\n          1.98\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ldl_mmol_l\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.644655629786555,\n        \"min\": 0.88,\n        \"max\": 6.83,\n        \"num_unique_values\": 341,\n        \"samples\": [\n          1.73,\n          3.03\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"alt_u_l\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 26.927227155954963,\n        \"min\": 3.0,\n        \"max\": 740.0,\n        \"num_unique_values\": 292,\n        \"samples\": [\n          39.0,\n          139.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"bun_mmol_l\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.2643663894708783,\n        \"min\": 2.46,\n        \"max\": 17.73,\n        \"num_unique_values\": 550,\n        \"samples\": [\n          6.37,\n          4.17\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ccr_umol_l\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 16.629379179434633,\n        \"min\": 36.0,\n        \"max\": 307.0,\n        \"num_unique_values\": 366,\n        \"samples\": [\n          73.7,\n          64.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"year_of_followup\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.9194296645358246,\n        \"min\": 2.0013689254,\n        \"max\": 5.3086926762,\n        \"num_unique_values\": 711,\n        \"samples\": [\n          4.0273785079,\n          2.2176591376\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"diabetes\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "diabetes_df_raw"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DY4rIR8e9SlO"
      },
      "source": [
        "# Session 1: Exploring the data and fitting a basic model\n",
        "\n",
        "Here, we will explore a given dataset related to diabetes and fit a basic model using the scikit-learn package.\n",
        "\n",
        "In particular, we aim to\n",
        " - understand the dataset, its variables and their relationship.\n",
        " - introduce the pandas-profiling and scikit-learn packages.\n",
        " - demonstrate how to fit a model using sklearn and look at the outputs.\n",
        "\n",
        "## Background: Dataset\n",
        "\n",
        "\n",
        "There are 17 measurements in the original study that we make use of here.\n",
        "\n",
        "\n",
        "| Column Name              | Meaning                                   | Units                                    |\n",
        "|--------------------------|-------------------------------------------|------------------------------------------|\n",
        "| Age (y)                  | Age of the participant                    | Years                                    |\n",
        "| Gender(1, male; 2, female)| Gender of the participant                | Categorical (1 = Male, 2 = Female)       |\n",
        "| site                     | Study site/location                       | Categorical                              |\n",
        "| height(cm)               | Height of the participant                 | Centimeters (cm)                         |\n",
        "| weight(kg)               | Weight of the participant                 | Kilograms (kg)                           |\n",
        "| z                        | Body Mass Index (unstandardized)          | Kilograms per Square Meter (kg/m²)       |\n",
        "| SBP(mmHg)                | Systolic Blood Pressure                   | Millimeters of Mercury (mmHg)            |\n",
        "| DBP(mmHg)                | Diastolic Blood Pressure                  | Millimeters of Mercury (mmHg)            |\n",
        "| FPG (mmol/L)             | Fasting Plasma Glucose                    | Millimoles per Liter (mmol/L)            |\n",
        "| Cholesterol(mmol/L)      | Total Cholesterol                         | Millimoles per Liter (mmol/L)            |\n",
        "| Triglyceride(mmol/L)     | Triglycerides                             | Millimoles per Liter (mmol/L)            |\n",
        "| HDL-c(mmol/L)            | High-Density Lipoprotein Cholesterol      | Millimoles per Liter (mmol/L)            |\n",
        "| LDL(mmol/L)              | Low-Density Lipoprotein                   | Millimoles per Liter (mmol/L)            |\n",
        "| ALT(U/L)                 | Alanine Aminotransferase                  | Units per Liter (U/L)                    |\n",
        "| AST(U/L)                 | Aspartate Aminotransferase                | Units per Liter (U/L)                    |\n",
        "| BUN(mmol/L)              | Blood Urea Nitrogen                       | Millimoles per Liter (mmol/L)            |\n",
        "| CCR(umol/L)              | Creatinine Clearance Rate                 | Micromoles per Liter (umol/L)            |\n",
        "\n",
        "Additionally, when loading in the data, we will be exploring the impact of randomly generated features that are not associated with your outcome. Such measurements are often generated in high-throughput 'omics studies where many things are measured that are not known to be associated.\n",
        "\n",
        "### Analysis aim\n",
        "\n",
        "The analysis goals from this dataset were \"inference\" based, i.e. trying to understand a relationship:\n",
        "\n",
        "> We investigated the association between body mass index (BMI) and diabetes across a wide range of age groups.\n",
        "\n",
        "For the purposes of this workshop, we'll assume the equivalent predictive question **\"does BMI improve prediction of incident diabetes between 2 and 7 years from onset beyond age, sex, and blood glucose?\"**. Now we have a specific baseline we can evaluate against.\n",
        "\n",
        "As a secondary goal, we'll investigate the maximum predictive performance we can achieve with this dataset, with a strong focus on generalization, i.e that the prediction will hold for new data.\n",
        "\n",
        "For those who are interested, the full study is available at https://datadryad.org/stash/dataset/doi:10.5061/dryad.ft8750v\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciSrEHOUNOa-"
      },
      "source": [
        "## 1.1 Load in the data\n",
        "We will load in a cleaned-up version of the dataset (using the *load_diabetes_data()* function)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LIpL_BVn9SlP",
        "outputId": "577d3105-9910-4614-951f-e7bcce657716"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/workshop_fn.py:90: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
            "  .groupby('diabetes')\n"
          ]
        }
      ],
      "source": [
        "# This cell creates of the dataframe that has been entered.\n",
        "X,y,X_ext,y_ext = workshop_fn.load_diabetes_data(diabetes_df_raw, add_n_features=10, sample_limit=1000, scale=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwVH1-DB9SlR"
      },
      "source": [
        "## 1.2 Data exploration\n",
        "We'll begin by exploring the data that is available. While we have a description of the fields, understanding the relationships between individual features and their relationship with the outcome of interest is informative for helping to understand downstream.\n",
        "\n",
        "Rather than try to generate a bunch of plots manually, we can make use of a package called *pandas-profiling*, which provides a bunch of handy plots.\n",
        "\n",
        "*This will take a minute or two to run*.\n",
        "\n",
        "After that a small report below will be generated that provides an overview of the different features and their relationships.\n",
        "\n",
        "**Examine the following:**\n",
        "\n",
        "1. What are the different types of features - which are numerical? which are categorical? are any unclear?\n",
        "2. Which features are correlated with each other? How strong are these?\n",
        "3. Is there anything unexpected about the data?\n",
        "4. Is there any missing data?\n",
        "5. What does the target variable look like? What is its distribution? Are there any obvious relationships?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 851
        },
        "id": "mT9selOQ8Obm",
        "outputId": "dca41b89-5b7a-455e-8f83-a593db6ea579"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                 Grouped by site                                          \n",
              "                                         Missing       Overall         site1         site2\n",
              "n                                                         1413          1000           413\n",
              "age_y, mean (SD)                               0   55.3 (13.1)   57.4 (12.9)   50.0 (12.0)\n",
              "gender_1_male_2_female, n (%)  1                   1021 (72.3)    684 (68.4)    337 (81.6)\n",
              "                               2                    392 (27.7)    316 (31.6)     76 (18.4)\n",
              "height_cm, mean (SD)                           0   166.0 (8.6)   165.4 (8.8)   167.5 (7.9)\n",
              "weight_kg, mean (SD)                           0   69.5 (12.3)   69.2 (12.5)   70.3 (11.8)\n",
              "bmi_kg_m2, mean (SD)                           0    25.1 (3.4)    25.2 (3.5)    25.0 (3.1)\n",
              "sbp_mm_hg, mean (SD)                           0  129.0 (18.8)  131.3 (19.5)  123.5 (15.7)\n",
              "dbp_mm_hg, mean (SD)                           0   79.3 (11.5)   79.9 (11.9)   77.9 (10.3)\n",
              "fpg_mmol_l, mean (SD)                          0     5.6 (0.7)     5.6 (0.7)     5.6 (0.6)\n",
              "cholesterol_mmol_l, mean (SD)                  0     5.0 (0.9)     5.0 (0.9)     5.0 (0.9)\n",
              "triglyceride_mmol_l, mean (SD)                 0     1.8 (1.3)     1.8 (1.2)     1.9 (1.4)\n",
              "hdl_c_mmol_l, mean (SD)                        0     1.4 (0.3)     1.4 (0.2)     1.3 (0.3)\n",
              "ldl_mmol_l, mean (SD)                          0     2.8 (0.7)     2.8 (0.7)     2.8 (0.6)\n",
              "alt_u_l, mean (SD)                             0   29.4 (27.1)   28.2 (26.3)   32.1 (29.0)\n",
              "bun_mmol_l, mean (SD)                          0     5.0 (1.2)     5.0 (1.2)     5.0 (1.1)\n",
              "ccr_umol_l, mean (SD)                          0   76.8 (15.1)   77.5 (15.8)   75.1 (13.1)\n",
              "simulated_0, mean (SD)                         0     0.0 (1.0)     0.0 (1.0)     0.0 (1.0)\n",
              "simulated_1, mean (SD)                         0    -0.0 (1.0)    -0.0 (1.0)     0.0 (1.0)\n",
              "simulated_2, mean (SD)                         0     0.0 (1.0)     0.0 (1.0)     0.0 (1.0)\n",
              "simulated_3, mean (SD)                         0     0.0 (1.0)     0.0 (1.0)    -0.0 (1.0)\n",
              "simulated_4, mean (SD)                         0     0.0 (1.0)     0.0 (1.0)     0.0 (1.0)\n",
              "y, n (%)                       0                    774 (54.8)    500 (50.0)    274 (66.3)\n",
              "                               1                    639 (45.2)    500 (50.0)    139 (33.7)"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr th {\n",
              "        text-align: left;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th colspan=\"4\" halign=\"left\">Grouped by site</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>Missing</th>\n",
              "      <th>Overall</th>\n",
              "      <th>site1</th>\n",
              "      <th>site2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>n</th>\n",
              "      <th></th>\n",
              "      <td></td>\n",
              "      <td>1413</td>\n",
              "      <td>1000</td>\n",
              "      <td>413</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_y, mean (SD)</th>\n",
              "      <th></th>\n",
              "      <td>0</td>\n",
              "      <td>55.3 (13.1)</td>\n",
              "      <td>57.4 (12.9)</td>\n",
              "      <td>50.0 (12.0)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">gender_1_male_2_female, n (%)</th>\n",
              "      <th>1</th>\n",
              "      <td></td>\n",
              "      <td>1021 (72.3)</td>\n",
              "      <td>684 (68.4)</td>\n",
              "      <td>337 (81.6)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td></td>\n",
              "      <td>392 (27.7)</td>\n",
              "      <td>316 (31.6)</td>\n",
              "      <td>76 (18.4)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>height_cm, mean (SD)</th>\n",
              "      <th></th>\n",
              "      <td>0</td>\n",
              "      <td>166.0 (8.6)</td>\n",
              "      <td>165.4 (8.8)</td>\n",
              "      <td>167.5 (7.9)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_kg, mean (SD)</th>\n",
              "      <th></th>\n",
              "      <td>0</td>\n",
              "      <td>69.5 (12.3)</td>\n",
              "      <td>69.2 (12.5)</td>\n",
              "      <td>70.3 (11.8)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bmi_kg_m2, mean (SD)</th>\n",
              "      <th></th>\n",
              "      <td>0</td>\n",
              "      <td>25.1 (3.4)</td>\n",
              "      <td>25.2 (3.5)</td>\n",
              "      <td>25.0 (3.1)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sbp_mm_hg, mean (SD)</th>\n",
              "      <th></th>\n",
              "      <td>0</td>\n",
              "      <td>129.0 (18.8)</td>\n",
              "      <td>131.3 (19.5)</td>\n",
              "      <td>123.5 (15.7)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>dbp_mm_hg, mean (SD)</th>\n",
              "      <th></th>\n",
              "      <td>0</td>\n",
              "      <td>79.3 (11.5)</td>\n",
              "      <td>79.9 (11.9)</td>\n",
              "      <td>77.9 (10.3)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fpg_mmol_l, mean (SD)</th>\n",
              "      <th></th>\n",
              "      <td>0</td>\n",
              "      <td>5.6 (0.7)</td>\n",
              "      <td>5.6 (0.7)</td>\n",
              "      <td>5.6 (0.6)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>cholesterol_mmol_l, mean (SD)</th>\n",
              "      <th></th>\n",
              "      <td>0</td>\n",
              "      <td>5.0 (0.9)</td>\n",
              "      <td>5.0 (0.9)</td>\n",
              "      <td>5.0 (0.9)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>triglyceride_mmol_l, mean (SD)</th>\n",
              "      <th></th>\n",
              "      <td>0</td>\n",
              "      <td>1.8 (1.3)</td>\n",
              "      <td>1.8 (1.2)</td>\n",
              "      <td>1.9 (1.4)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>hdl_c_mmol_l, mean (SD)</th>\n",
              "      <th></th>\n",
              "      <td>0</td>\n",
              "      <td>1.4 (0.3)</td>\n",
              "      <td>1.4 (0.2)</td>\n",
              "      <td>1.3 (0.3)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ldl_mmol_l, mean (SD)</th>\n",
              "      <th></th>\n",
              "      <td>0</td>\n",
              "      <td>2.8 (0.7)</td>\n",
              "      <td>2.8 (0.7)</td>\n",
              "      <td>2.8 (0.6)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>alt_u_l, mean (SD)</th>\n",
              "      <th></th>\n",
              "      <td>0</td>\n",
              "      <td>29.4 (27.1)</td>\n",
              "      <td>28.2 (26.3)</td>\n",
              "      <td>32.1 (29.0)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bun_mmol_l, mean (SD)</th>\n",
              "      <th></th>\n",
              "      <td>0</td>\n",
              "      <td>5.0 (1.2)</td>\n",
              "      <td>5.0 (1.2)</td>\n",
              "      <td>5.0 (1.1)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ccr_umol_l, mean (SD)</th>\n",
              "      <th></th>\n",
              "      <td>0</td>\n",
              "      <td>76.8 (15.1)</td>\n",
              "      <td>77.5 (15.8)</td>\n",
              "      <td>75.1 (13.1)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>simulated_0, mean (SD)</th>\n",
              "      <th></th>\n",
              "      <td>0</td>\n",
              "      <td>0.0 (1.0)</td>\n",
              "      <td>0.0 (1.0)</td>\n",
              "      <td>0.0 (1.0)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>simulated_1, mean (SD)</th>\n",
              "      <th></th>\n",
              "      <td>0</td>\n",
              "      <td>-0.0 (1.0)</td>\n",
              "      <td>-0.0 (1.0)</td>\n",
              "      <td>0.0 (1.0)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>simulated_2, mean (SD)</th>\n",
              "      <th></th>\n",
              "      <td>0</td>\n",
              "      <td>0.0 (1.0)</td>\n",
              "      <td>0.0 (1.0)</td>\n",
              "      <td>0.0 (1.0)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>simulated_3, mean (SD)</th>\n",
              "      <th></th>\n",
              "      <td>0</td>\n",
              "      <td>0.0 (1.0)</td>\n",
              "      <td>0.0 (1.0)</td>\n",
              "      <td>-0.0 (1.0)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>simulated_4, mean (SD)</th>\n",
              "      <th></th>\n",
              "      <td>0</td>\n",
              "      <td>0.0 (1.0)</td>\n",
              "      <td>0.0 (1.0)</td>\n",
              "      <td>0.0 (1.0)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">y, n (%)</th>\n",
              "      <th>0</th>\n",
              "      <td></td>\n",
              "      <td>774 (54.8)</td>\n",
              "      <td>500 (50.0)</td>\n",
              "      <td>274 (66.3)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td></td>\n",
              "      <td>639 (45.2)</td>\n",
              "      <td>500 (50.0)</td>\n",
              "      <td>139 (33.7)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br />"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "x1 = X.iloc[:, 0:20].assign(y=y, site='site1')\n",
        "x2 = X_ext.iloc[:, 0:20].assign(y=y_ext, site='site2')\n",
        "# Concatenate the two DataFrames\n",
        "combined_df = pd.concat([x1, x2], axis=0)\n",
        "\n",
        "# Reset the index if necessary\n",
        "combined_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "tab1 = TableOne(data=combined_df, groupby='site')\n",
        "tab1\n",
        "# Print the table with formatting options\n",
        "#print(tab1.tabulate(tablefmt=\"plain\", floatfmt=\".2f\", numalign=\"right\", stralign=\"left\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "zY6W_IAa9SlS"
      },
      "outputs": [],
      "source": [
        "# Ignore most of the simulated features for this exploration\n",
        "# Also only use 200 samples so that we don't crash\n",
        "\n",
        "X_y = X.iloc[1:200, 0:20].assign(y = y)\n",
        "\n",
        "profile = ProfileReport(X_y,\n",
        "                        correlations={\n",
        "          \"pearson\": {\"calculate\": False},\n",
        "          \"spearman\": {\"calculate\": True},\n",
        "          \"kendall\": {\"calculate\": False},\n",
        "          \"phi_k\": {\"calculate\": True},\n",
        "          \"cramers\": {\"calculate\": False},\n",
        "      })\n",
        "\n",
        "profile.to_notebook_iframe()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "profile.to_widgets()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "c2ae599412cc4b41baea9dd763035f91",
            "f3e1a919ad43413581e2ae081a6bf145",
            "2e10c9128dda441b98a89e7c82b6007a",
            "3f162a4dd5274d75b5eb4a27b2379bea",
            "37867064a205462a9e48c83857965ccb",
            "f570bd61e736466ba935fe1e5d58141d",
            "7ac5288e99794e059f3a9d2694e88587",
            "fcf493e78bf443c9b6a7b7b07fd3144c",
            "a8a1daecea794d5d87c3bf445668516c",
            "1330f754f72c45ceb593d09b35b6492f",
            "0d0f52db068f4ceca798a586e438a5fa"
          ]
        },
        "id": "wfi0OGM0RcG4",
        "outputId": "e5f4761a-7bed-4646-c978-71f5e9668fd3"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Render widgets:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c2ae599412cc4b41baea9dd763035f91"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GeM844D09SlT"
      },
      "source": [
        "## 1.3 Fitting a model to the data\n",
        "\n",
        "Lets fit a simple logistic regression to the data and look at how well it makes predictions on the data. The code below will fit a simple logistic regression , using only a single predictor (bmi), to allow for simple plots of the fit, and then using all available features.\n",
        "\n",
        "We begin by plotting the data, the model fit and some classic summary statistics.\n",
        "\n",
        "*Questions*:\n",
        "1.  Which features are the most predictive?\n",
        "2.  How much improvement to you get if you combined features compared to a model based on individual features?\n",
        "3.  Try regenerating the dataset with more or less simulated. What happens to prediction accuracy as you add more noisy variables?\n",
        "4. The `scale` parameter in  load_diabetes_data determines whether we normalize our variables so they have a mean of 0 and a standard deviation of 1. What happens when we set this to True or False?\n",
        "\n",
        " Note: the data has a bunch of variables called simulated<number> e.g simulated_1, simulated_2 etc. These are just randomly generated numbers. But in real life these types of variables do exist - they are essentially any variable that is unrelated to the thing we are trying to predict. In this data, we explicitly know these simulated variables but in your own datasets (and for the other variables in the diabetes dataset), we have no idea which variables are related and which are not.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2F0ps1rO9SlX",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "\n",
        "# If you feel like generating a new dataset, uncomment this line.\n",
        "X,y,X_ext,y_ext = workshop_fn.load_diabetes_data(diabetes_df_raw, add_n_features=100, sample_limit=1000, scale=True)\n",
        "\n",
        "# The features used to construct the model\n",
        "# Change these and see how the results change\n",
        "# This option would include all features\n",
        "features= X.columns\n",
        "\n",
        "# We can also look at models with BMI and one other feature\n",
        "#features=[\"bmi_kg_m2\", \"fpg_mmol_l\"]\n",
        "#features=[\"bmi_kg_m2\", \"age_y\"]\n",
        "#features=[\"bmi_kg_m2\", \"simulated_1\"]\n",
        "\n",
        "# This set will include 4 features\n",
        "#features=[\"age_y\", \"gender_1_male_2_female\", \"bmi_kg_m2\", \"fpg_mmol_l\"]\n",
        "\n",
        "\n",
        "# sklearn models expect a 2D array. However, when only a single column is selected in pandas, it is a\n",
        "# 1D array. This conditional checks for when our input is a single column and turns it into a 2D array for sklearn.\n",
        "if type(features)==str:\n",
        "    X_train    = X.loc[:, features].values.reshape(-1,1)\n",
        "    X_eval = X_ext.loc[:, features].values.reshape(-1,1)\n",
        "else:\n",
        "    X_train    = X.loc[:, features]\n",
        "    X_eval = X_ext.loc[:, features]\n",
        "\n",
        "#Construct a classifier\n",
        "clf = LogisticRegression(penalty=None, solver=\"saga\", tol=0.01)\n",
        "\n",
        "clf_w_bmi = clone(clf).fit(X_train, y)\n",
        "clf_no_bmi = clone(clf).fit(X_train.drop(columns=['bmi_kg_m2']), y)\n",
        "\n",
        "# Get predicted labels from the classifier. Here, the 'predict_proba' function returns probabilities of the labels (e.g 75% of belonging to class 1)\n",
        "# get predictions when given the training data\n",
        "y_pred = clf_w_bmi.predict_proba(X_train)[:,1]\n",
        "y_pred_no_bmi = clf_no_bmi.predict_proba(X_train.drop(columns=['bmi_kg_m2']))[:,1]\n",
        "\n",
        "# get predictions when given the evaluation data\n",
        "y_ext_pred = clf_w_bmi.predict_proba(X_eval)[:,1]\n",
        "y_ext_pred_no_bmi = clf_no_bmi.predict_proba(X_eval.drop(columns=['bmi_kg_m2']))[:,1]\n",
        "\n",
        "\n",
        "fig, ax=plt.subplots(nrows=1,ncols=2, figsize=(10,5), dpi= 100, facecolor='w', edgecolor='k')\n",
        "\n",
        "# PLot a ROC curve and show the area under the curve.\n",
        "workshop_fn.plot_roc(y, y_pred, ax=ax[0], label=\"Study1 (Internal) - With BMI\")\n",
        "workshop_fn.plot_roc(y, y_pred_no_bmi, ax=ax[0], label=\"Study1 (Internal) - No BMI\", color='red')\n",
        "workshop_fn.plot_roc(y_ext, y_ext_pred, ax=ax[1], label=\"Study2 (external) - With BMI\")\n",
        "workshop_fn.plot_roc(y_ext, y_ext_pred_no_bmi, ax=ax[1], label=\"Study2 (external)- No BMI\", color='red')\n",
        "ax[0].legend(loc=0)\n",
        "ax[0].title.set_text('Study1 (internal)')\n",
        "ax[1].legend(loc=0)\n",
        "ax[1].title.set_text('Study2 (external)')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFu4xZtE9SlY"
      },
      "source": [
        "## 1.4 Examine top features\n",
        "Logistic regression provides an interpretable model. To see which features are most important, we can look at the coefficients of each variable. Larger absolute values implies more impact in the predictions.\n",
        "\n",
        "\n",
        "**Examine the following:**\n",
        "\n",
        "1. Do any simulated variables make it into the top 10?\n",
        "2. What if you generate lots of simulated variables (>1000)? What if we have less?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DHG7UPQM9SlZ"
      },
      "outputs": [],
      "source": [
        "coefs_df = pd.DataFrame.from_dict({'feature':features, 'coef':clf_w_bmi.coef_[0]})\n",
        "coefs_df.sort_values(by=\"coef\", key=np.abs, ascending=False).iloc[0:15, :]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOk3odLG8Obn"
      },
      "source": [
        "<hr style=\"border:2px solid gray\"> </hr>\n",
        "\n",
        "# Session 2: models and metrics in sklearn\n",
        "\n",
        "In the previous example, we fit a logistic regression model to the given dataset and examine its performance using AUC. However, in many studies where we are looking to create a predictive model, we will be interested in creating multiple models based on different underlying algorithms and possibly evaluating them based on different criteria. Here, we demonstrate how these different models and metrics can be called and provide a few examples with our diabetes dataset as to the information you get.      "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ig8kop3o8Obn"
      },
      "source": [
        "## 2.1 Exploring different models\n",
        "The sklearn has quite a standardised interface for fitting and applying different models. In particular clf.fit(X, y) and clf.predict_proba(X) can be used to fit a model and then extract the probabilities of the predicted classes. Comparable functions exist when looking at continuous or multi-label outcomes.\n",
        "\n",
        "These standardised interfaces allow us to easily explore the impact of different classifiers for a problem. UNderstanding the different assumptions and methods is beyond this workshop. However, we can explore how this is done and talk through the impact of diffent classifiers.\n",
        "\n",
        "\n",
        "**Examine the following:**\n",
        "\n",
        "1. How well do different classifiers perform on the given dataset? Which models maximise performance on the training set? What does external performance look like?\n",
        "2. Are there noticable timing differences?\n",
        "3. Model configuration can have a big impact. For LogisticRegression(penalty='l1'), try changing the C to 1e-1, 1e-2, and 1e-3. How does performance change?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m5byVxvu8Obn"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "#X,y,X_ext,y_ext = workshop_fn.load_diabetes_data(diabetes_df_raw, add_n_features=100, sample_limit=1000, scale=True)\n",
        "\n",
        "\n",
        "#\n",
        "# Change these options to change your classifier\n",
        "#\n",
        "#clf=LogisticRegression(penalty=None, solver=\"saga\", tol=0.01)\n",
        "clf=LogisticRegression(penalty='l1',C=1e-2, solver='saga')\n",
        "#clf=RandomForestClassifier(max_depth=1, random_state=0)\n",
        "#clf=DecisionTreeClassifier(max_depth=None, random_state=0)\n",
        "\n",
        "\n",
        "\n",
        "# This code is all the same as in Section 1.3\n",
        "clf_w_bmi = clone(clf).fit(X_train, y)\n",
        "clf_no_bmi = clone(clf).fit(X_train.drop(columns=['bmi_kg_m2']), y)\n",
        "\n",
        "# Get predicted labels from the classifier. Here, the 'predict_proba' function returns probabilities of the labels (e.g 75% of belonging to class 1)\n",
        "# get predictions when given the training data\n",
        "y_pred = clf_w_bmi.predict_proba(X_train)[:,1]\n",
        "y_pred_no_bmi = clf_no_bmi.predict_proba(X_train.drop(columns=['bmi_kg_m2']))[:,1]\n",
        "\n",
        "# get predictions when given the evaluation data\n",
        "y_ext_pred = clf_w_bmi.predict_proba(X_eval)[:,1]\n",
        "y_ext_pred_no_bmi = clf_no_bmi.predict_proba(X_eval.drop(columns=['bmi_kg_m2']))[:,1]\n",
        "\n",
        "\n",
        "fig, ax=plt.subplots(nrows=1,ncols=2, figsize=(10,5), dpi= 100, facecolor='w', edgecolor='k')\n",
        "\n",
        "# PLot a ROC curve and show the area under the curve.\n",
        "workshop_fn.plot_roc(y, y_pred, ax=ax[0], label=\"Study1 (Internal) - With BMI\")\n",
        "workshop_fn.plot_roc(y, y_pred_no_bmi, ax=ax[0], label=\"Study1 (Internal) - No BMI\", color='red')\n",
        "workshop_fn.plot_roc(y_ext, y_ext_pred, ax=ax[1], label=\"Study2 (external) - With BMI\")\n",
        "workshop_fn.plot_roc(y_ext, y_ext_pred_no_bmi, ax=ax[1], label=\"Study2 (external)- No BMI\", color='red')\n",
        "ax[0].legend(loc=0)\n",
        "ax[0].title.set_text('Study1 (internal)')\n",
        "ax[1].legend(loc=0)\n",
        "ax[1].title.set_text('Study2 (external)')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uoa5Oc0j8Obo"
      },
      "source": [
        "## 2.2 Example of a specific model output\n",
        "Different machine learning methods have very different underlying algorithms and can produce very different outputs.\n",
        "\n",
        "Below we take an example at a unique output, that of a decision tree, to peek into how it works. Here, the model produces a sort of flowchart for how a sample should be classifier, based on a series of binary decisions. We visualise this model below where each node shows\n",
        "1. A variable chosen to make a decision and a threshold for which side of the subtree we go down\n",
        "2. gini value, a measure of how well the given split separates classes (lower is more discriminative)\n",
        "3. samples is the number of samples in a node in the training data\n",
        "4. values is the number of samples in each class in the training data (sum of 'values' is the same as 'samples')\n",
        "\n",
        "**Examine the following:**\n",
        "1. What happens when we have more or less noise variables in the data? Do they make it into the tree?\n",
        "2. What if we make the tree bigger or smaller (n=1 or n=5)? How do you think the model will perform\n",
        "3. Try make the max_depth=7. Is the tree still interpretable? How many noise variables are included"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kLlryAOl8Obo"
      },
      "outputs": [],
      "source": [
        "from sklearn import tree\n",
        "\n",
        "#Fit a decision tree.\n",
        "#   Max depth controls how many rules/layers your tree has.\n",
        "#   Random state is so that the random part of the tree is replicable\n",
        "clf=DecisionTreeClassifier(max_depth=3, random_state=0)\n",
        "clf.fit(X_train, y)\n",
        "\n",
        "#\n",
        "# These parameters control the size of the resulting figure. If your model is too deep, you may need to make these values larger\n",
        "plt.figure(figsize=(30,20))\n",
        "tree.plot_tree(clf, feature_names = X_train.columns, fontsize=12)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbhi8PPz8Obo"
      },
      "source": [
        "## 2.3 Exploring different metrics\n",
        "\n",
        "We can also change the way that we evaluate model performance, again through the standardised interface provided by scikit-learn\n",
        "\n",
        "A list of possible options are provided can be see at\n",
        "https://scikit-learn.org/0.16/modules/model_evaluation.html\n",
        "\n",
        "Examine the following:\n",
        "\n",
        "1. Do different metrics ever change the ranking of which methods are best?\n",
        "2. How do results compare on the internal and external predictions? Are they the same? Do they dramatically differ?\n",
        "3. What happens when you adjust the amount of noise variables in the dataset?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BdGQI0mg8Obo"
      },
      "outputs": [],
      "source": [
        "# If you feel like generating a new dataset, uncomment this line.\n",
        "#X,y,X_ext,y_ext = workshop_fn.load_diabetes_data(diabetes_df_raw, add_n_features=100, sample_limit=1000, scale=True)\n",
        "\n",
        "# Create a dictionary of multiple classifiers\n",
        "# Use the scikit learn website to try and learn more about the parameters\n",
        "#  https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
        "#  https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
        "clfs = {\n",
        "    'Ridge':LogisticRegression(penalty='l2',C=10, tol=0.01),\n",
        "    'LogisticReg':LogisticRegression(penalty=None, solver=\"saga\", tol=0.01),\n",
        "    'Lasso':LogisticRegression(penalty='l1', C=0.01, solver=\"saga\", tol=0.01),\n",
        "    'RandomForest':RandomForestClassifier(random_state=0, n_estimators=500),\n",
        "    'DecisionTree':DecisionTreeClassifier(max_depth=5, random_state=0)\n",
        "}\n",
        "\n",
        "#Different metrics we might be interested in\n",
        "metrics_dict = {\n",
        "    'Accuracy':metrics.accuracy_score,\n",
        "    'BalAccuracy':metrics.balanced_accuracy_score,\n",
        "    'AUC':metrics.roc_auc_score,\n",
        "    'LogLoss':metrics.log_loss,\n",
        "    'PPV':metrics.precision_score,\n",
        "    'NPV':metrics.precision_score,\n",
        "}\n",
        "\n",
        "#Helper function to return classifier name\n",
        "def get_clf_name(estimator):\n",
        "    return(estimator.__class__.__name__)\n",
        "\n",
        "scores = {}\n",
        "for clf_name,clf in clfs.items():\n",
        "    scores[clf_name] = {}\n",
        "    clf.fit(X, y)\n",
        "    for metric_name, metric in metrics_dict.items():\n",
        "        if metric_name==\"AUC\":\n",
        "            scores[clf_name][metric_name + '_Internal']=metric(y, clf.predict_proba(X)[:,1])\n",
        "            scores[clf_name][metric_name + '_External']=metric(y_ext, clf.predict_proba(X_ext)[:,1])\n",
        "        else:\n",
        "            scores[clf_name][metric_name + '_Internal']=metric(y, clf.predict(X))\n",
        "            scores[clf_name][metric_name + '_External']=metric(y_ext, clf.predict(X_ext))\n",
        "\n",
        "pd.options.display.float_format = '{:,.2f}'.format\n",
        "pd.DataFrame(scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tdicVuS9SlZ",
        "scrolled": true
      },
      "source": [
        "<hr style=\"border:2px solid gray\"> </hr>\n",
        "\n",
        "# Session 3: Train/test and cross-validation frameworks\n",
        "\n",
        "In the previous example, we built model on entire dataset and evaluated its performance on the same data. Here, we will explore some alternative frameworks for doing this and will evaluate how model performance changes. We'll also start to explore different models and how key parameters can be altered to change prediction performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4oWEBEBk9Sla"
      },
      "source": [
        "## 3.1 Training and Test split\n",
        "\n",
        "The following code examines training and testing a model on a single dataset and compares its performance to an external dataset in 3 scenarios:\n",
        " - Training on the entire dataset. Test on the same entire dataset.\n",
        " - Training on a proportion (default 80%). Test on the same proportion.\n",
        " - Training on a proportion (default 80%). Test on the remaining proportion.\n",
        "\n",
        "**Questions:**\n",
        "1. Run this cell a few times. Which accuracies change? Why?\n",
        "2. Which evaluation scenario is closest to the external data performance?\n",
        "2. What is the best performance you can get by default? What is the worst performance you observe?\n",
        "3. What happens to test performance as you add more noise variables?\n",
        "4. Try changing parameters ('C' for the l2 penalized logistic regression, or max_depth for Random Forest)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CpZ7Q5jx9Sla"
      },
      "outputs": [],
      "source": [
        "# If you feel like generating a new dataset, uncomment this line.\n",
        "#X,y,X_ext,y_ext = workshop_fn.load_diabetes_data(diabetes_df_raw, add_n_features=100, sample_limit=1000, scale=True)\n",
        "\n",
        "#\n",
        "# Select a model to construct\n",
        "#\n",
        "#clf=LogisticRegression(penalty=None, solver=\"saga\", tol=0.01)\n",
        "#clf=LogisticRegression(penalty='l2',C=10, tol=0.01)\n",
        "clf=RandomForestClassifier(max_depth=None, random_state=0)\n",
        "\n",
        "# Propotion of data to use for testing\n",
        "test_prop = 0.2\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_prop)\n",
        "\n",
        "scenarios = {\n",
        "                \"Tr:all_Te:all\":    {\"train\":{'X':X,'y':y}, \"test\":{'X':X,'y':y}},\n",
        "                \"Tr:train_Te:train\":  {\"train\":{'X':X_train,'y':y_train}, \"test\":{'X':X_train,'y':y_train}},\n",
        "                \"Tr:train_Te:test\":   {\"train\":{'X':X_train,'y':y_train}, \"test\":{'X':X_test,'y':y_test}}\n",
        "            }\n",
        "\n",
        "fig, ax=plt.subplots(nrows=1,ncols=2, figsize=(10,5), dpi= 100, facecolor='w', edgecolor='k')\n",
        "\n",
        "# Fit the model on the internal data, make predictions on whatever we are calling the test data and plot the results\n",
        "colours={\n",
        "    \"Tr:all_Te:all\":'blue',\n",
        "    \"Tr:train_Te:train\":'red',\n",
        "    \"Tr:train_Te:test\":'green'\n",
        "}\n",
        "for name, sc in scenarios.items():\n",
        "    clf.fit(sc['train']['X'], sc['train']['y'])\n",
        "\n",
        "    yp=clf.predict_proba(sc['test']['X'])[:,1]\n",
        "    workshop_fn.plot_roc(sc['test']['y'], yp, name, ax=ax[0], color=colours[name])\n",
        "\n",
        "# Fit the model on the external data and plot the results\n",
        "clf.fit(X, y)\n",
        "yp_ext=clf.predict_proba(X_ext)[:,1]\n",
        "workshop_fn.plot_roc(y_ext, yp_ext, \"External\", ax=ax[1])\n",
        "\n",
        "ax[0].legend(loc=0)\n",
        "ax[0].title.set_text('Internal')\n",
        "ax[1].legend(loc=0)\n",
        "ax[1].title.set_text('External')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJu0PpDB9Slb"
      },
      "source": [
        "## 3.2 K-fold Cross-validation\n",
        "Variability in the performance of different splits in the previous example motivates the use of K-fold cross validation. Here, we explore a few models and start to compare model performance.  \n",
        "\n",
        "***Warning***: Be careful setting the values below. Setting the number of times to evaluate the classifiers too high and it will take too long to run for this workshop.\n",
        "\n",
        "**Questions:**\n",
        "1. Run this cell a few times. What is the range of the scores that are observed?\n",
        "2. Which model is the best? How do you determine this?\n",
        "3. Play around with hyperparameters, what is the impact on model performance? Which models are sensitive to these choices?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C11BomI69Slc"
      },
      "outputs": [],
      "source": [
        "# If you feel like generating a new dataset, uncomment this line.\n",
        "#X,y,X_ext,y_ext = workshop_fn.load_diabetes_data(diabetes_df_raw, add_n_features=100, sample_limit=1000, scale=True)\n",
        "\n",
        "clfs = {\n",
        "    'ridge':LogisticRegression(penalty='l2',C=10, tol=0.01),\n",
        "    'logreg':LogisticRegression(penalty=None, solver=\"saga\", tol=0.01),\n",
        "    'lasso':LogisticRegression(penalty='l1', C=100, solver=\"saga\", tol=0.01),\n",
        "    'RandomForest':RandomForestClassifier(max_depth=20, random_state=0),\n",
        "}\n",
        "\n",
        "def get_clf_name(estimator):\n",
        "    return(estimator.__class__.__name__)\n",
        "\n",
        "# This constructs n_splits * n_repeats classifiers. If these values are large,\n",
        "# or if classifier is slow it may take a long time\n",
        "n_folds=5\n",
        "n_reps=3\n",
        "cv = RepeatedKFold(n_splits=n_folds,n_repeats=n_reps)\n",
        "\n",
        "clfs_res=[]\n",
        "print(\"{} total classifiers: \".format(len(clfs.items())), end=\"\")\n",
        "for i, (clf_name, clf) in enumerate(clfs.items()):\n",
        "    print(i, end=\"\")\n",
        "\n",
        "    # Record the  AUC for this classifier\n",
        "    scores = cross_val_score(clf, X, y, cv=cv, scoring=\"roc_auc\")\n",
        "\n",
        "    #Turn the results into a data frame and add the classifier name\n",
        "    clf_res=pd.DataFrame(data = scores, columns = ['auc']).assign(clf=clf_name)\n",
        "    clfs_res.append(clf_res)\n",
        "print(\"Done\", end=\"\")\n",
        "\n",
        "#Make one big dataframe rather than a list of data.frames\n",
        "clfs_res_merge = pd.concat(clfs_res)\n",
        "\n",
        "#Now make a boxplot of the AUCs\n",
        "fig, ax=plt.subplots(nrows=1,ncols=1, figsize=(10,5), dpi= 100, facecolor='w', edgecolor='k')\n",
        "sns.boxplot(data=clfs_res_merge, y='auc', x=\"clf\",ax=ax )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7UftUfo9Sld"
      },
      "source": [
        "# Session 4. Overfitting via feature selection and model parameters\n",
        "\n",
        "\n",
        "## 4.1 Feature discrimination over entire dataset\n",
        "One naive way to remove noise is to look at the features one-by-one,  look at their ability to discriminate the dataset and only take the most useful into our model. This is flawed but is common in the literature.\n",
        "\n",
        "Lets take a look at the discriminatory ability of our features, here using an ANOVA, a common statistical test. We report the f-statistic (a measure of effect size) and p-value for each features.\n",
        "\n",
        "**Question:**\n",
        "1. How do measured and noisy simulated features compare?\n",
        "2. What if we generate lots (n=10,000) noisy simulated features? How often can we distinguish noise and real signal?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WpTcAPfM9Sld"
      },
      "outputs": [],
      "source": [
        "# If you feel like generating a new dataset, uncomment this line.\n",
        "#X,y,X_ext,y_ext = workshop_fn.load_diabetes_data(diabetes_df_raw, add_n_features=100, sample_limit=1000, scale=True)\n",
        "\n",
        "# Look at which features are important over the entire dataset\n",
        "f, p = feature_selection.f_classif(X, y)\n",
        "feature_scores = pd.DataFrame.from_dict({\"feature\":X.columns, \"f-stat\":f, \"p.val\":p})\n",
        "feature_scores.sort_values(by=\"p.val\", key=np.abs, ascending=True).iloc[0:10, :]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldM_WpdT9Sle"
      },
      "source": [
        "## 4.2 Demonstrating potential overfitting when selecting features before CV\n",
        "Given we've ranked the features in terms of their discrinatory ability, we could now select some top amount (based on p-value, f-statistic or a feeling for how many features we need).\n",
        "\n",
        "But such an approach uses all of the information, and hence means there is no unsed data left for an untouched test set.\n",
        "\n",
        "To explore this impact, the code below plots classifier performance starting with a single most discrinimatory feature and increasing to the top 32 features. We plot the model performance in training and testing.\n",
        "\n",
        "Additionally, we plot the performance of constructing a model on all samples and evaluating the external dataset to show where the ideal would be.\n",
        "\n",
        "**Questions**\n",
        "1. What are the trends in the performance of the model on the training data as we increase features?\n",
        "2. What are the trends in the performance of the model on the test data as we increase features?\n",
        "3. Where is the ideal number of features for the external data?\n",
        "4. How often does number of features to achieve the highest \"test\" performance correspond to the hihgest external performance?\n",
        "5. What happens if you run this cell a few times? How do results differ? Why do they change?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1bzTAixB9Sle"
      },
      "outputs": [],
      "source": [
        "#X,y,X_ext,y_ext = workshop_fn.load_diabetes_data(diabetes_df_raw, add_n_features=100, sample_limit=1000, scale=True)\n",
        "\n",
        "run_biased_analysis=True\n",
        "\n",
        "#\n",
        "# Pick your classifier\n",
        "#\n",
        "clf = LogisticRegression(penalty=None, tol=0.01, solver='saga')\n",
        "#clf = RandomForestClassifier(max_depth=20, random_state=0)\n",
        "\n",
        "\n",
        "# Set up a train/test split. You can change 'test_prop' to any (0-1) value\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_prop)\n",
        "\n",
        "#Look at different numbers of features in the model. Remove any that are bigger than our current dataframe\n",
        "n_feats = [1,2,3, 4,5, 6, 8,12,16,20, 24, 28, 32, 64, 128, 200, 1000]\n",
        "n_feats = filter(lambda x: x<=len(X_train.columns), n_feats)\n",
        "aucs=[]\n",
        "for i in n_feats:\n",
        "\n",
        "    # This is a clever trick, where we make a pipeline that first searches\n",
        "    # for the 'k' best features and then applies a classifier  to the filtered data.\n",
        "    #\n",
        "    # Here, the best is based on an ANOVA F-statistic\n",
        "    # https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_classif.html\n",
        "    #\n",
        "    # But you could use a range of other functions\n",
        "    # https://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection\n",
        "    clf_selected = make_pipeline(SelectKBest(f_classif, k=i),clf)\n",
        "\n",
        "    #\n",
        "    ## Two possibilities to choose from\n",
        "    if run_biased_analysis == True:\n",
        "        #   Biased: Use this to conduct feature selection on the entire dataset\n",
        "        clf_selected.fit(X, y)\n",
        "    else:\n",
        "        #   Unbiased: Use this to conduct feature selection only on the training data\n",
        "        clf_selected.fit(X_train, y_train)\n",
        "\n",
        "    aucs.append(pd.DataFrame.from_dict({\n",
        "        \"n_feat\": [i,i],\n",
        "        \"model\" : [\"Train\", \"Test\"],\n",
        "        \"auc\" : [clf_selected.score(X_train, y_train),\n",
        "                   clf_selected.score(X_test, y_test)]\n",
        "    }))\n",
        "\n",
        "    clf_selected.fit(X, y)\n",
        "    aucs.append(pd.DataFrame.from_dict({\n",
        "        \"n_feat\": [i],\n",
        "        \"model\" : [\"External\"],\n",
        "        \"auc\" : [clf_selected.score(X_ext, y_ext)]\n",
        "    }))\n",
        "\n",
        "\n",
        "\n",
        "aucs_df=pd.concat(aucs, ignore_index=True)\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "f=sns.lineplot(data=aucs_df, x=\"n_feat\", y=\"auc\", hue=\"model\")\n",
        "#f.set(xscale='log')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9LgLCyUG9Slf"
      },
      "source": [
        "## 4.3 Nested cross-validation for feature selection and hyperparameter tuning\n",
        "\n",
        "Rather than selecting the number of features to be included in a model (based on a random guess, or worse peeking at test results), we can instead conduct feature selection as part of cross validation.\n",
        "\n",
        "The code below runs two cross-validation loops (inner and outer), essentially running one loop on the training data (repeatededly breaking it into training and validation datasets) to understand how the number of features impacts performance. We then select the best number of features and evaluate the held-out testset. This is then repeated for the number of folds in the outer loop.\n",
        "\n",
        "While robust, the approach can be computationally expensive as we are building many models.\n",
        "\n",
        "**Warning** This code will take a few minutes. If you add in hyperparameter selection (by uncommenting param_grid),  this could take quite a while to run in Google Colab.\n",
        "\n",
        "**Questions:**\n",
        "1. Try generating a dataset with no noisy features and one with many? How much does performance vary?\n",
        "2. How does performance vary if we change from a penalized regression to a random forest?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EGKR08T_9Slf"
      },
      "outputs": [],
      "source": [
        "#X,y,X_ext,y_ext = workshop_fn.load_diabetes_data(diabetes_df_raw, add_n_features=100, sample_limit=1000, scale=True)\n",
        "\n",
        "# Form a grid of parameters to look over\n",
        "n_feats=[1,2,3, 4,5, 6, 8,12,16,20, 24, 28, 32]\n",
        "n_feats=list(filter(lambda x: x<=len(X_train.columns), n_feats))\n",
        "\n",
        "# Fit a regularised logistic regression\n",
        "clf = LogisticRegression(penalty='l2', solver=\"saga\", tol=0.01)\n",
        "# Fit a random forest\n",
        "#clf = RandomForestClassifier(max_depth=20, random_state=0)\n",
        "\n",
        "\n",
        "#This line only optimises # of features selected. Next two also optimise model parameters but I think this is too slow for colab.\n",
        "param_grid = {\"selectkbest__k\": n_feats}\n",
        "#param_grid = {\"selectkbest__k\": n_feats, \"logisticregression__C\": [0.01, 0.1, 1,5, 10, 50, 100]}\n",
        "#param_grid = {\"selectkbest__k\": n_feats, \"randomforestclassifier__max_depth\": [5, 20, 50, 100]}\n",
        "\n",
        "\n",
        "model_to_tune = make_pipeline(SelectKBest(f_classif),clf)\n",
        "\n",
        "test_score_not_nested = []\n",
        "test_score_nested = []\n",
        "\n",
        "n_rep = 3\n",
        "n_split_outer=5\n",
        "n_split_inner=3\n",
        "\n",
        "for i in range(n_rep):\n",
        "    print(i, end='')\n",
        "\n",
        "    #Set up two cross-validation helpers\n",
        "    # Could also use RepeatedKFold() here but quickly becomes slow to fit\n",
        "    inner_cv = KFold(n_splits=n_split_inner, shuffle=True, random_state=i)\n",
        "    outer_cv = KFold(n_splits=n_split_outer, shuffle=True, random_state=i)\n",
        "\n",
        "    # Warning: the code below is not very obvious\n",
        "\n",
        "    # 1. Evaluate Non-nested parameter search and scoring\n",
        "    #   GridSearchCV will take a model to fit and a grid of parameters and will evaluate performance\n",
        "    #   as a bunch of test/train splits. We can then select the model with the best score on the test data\n",
        "    #   But this is biased as we evaluate the model based on the test data.\n",
        "    model = GridSearchCV(estimator=model_to_tune, param_grid=param_grid, cv=inner_cv, scoring=\"roc_auc\")\n",
        "    model.fit(X, y)\n",
        "    test_score_not_nested.append(model.best_score_)\n",
        "\n",
        "    # 2. Evaluate Nested CV with parameter optimization\n",
        "    #     cross_val_score() will do all of our model fitting, first splitting the data into test and the remainder of the data then gets\n",
        "    #        split using GridSearchCV() into training and validation. The best model is selected from performance on the validation data\n",
        "    #         and this final model is used to evaluate the held-out test data.\n",
        "    # Unbiased as model selection takes place on the validation, rather than test data.\n",
        "    model = GridSearchCV(estimator=model_to_tune, param_grid=param_grid, cv=inner_cv, scoring=\"roc_auc\")\n",
        "    test_score = cross_val_score(model, X, y, cv=outer_cv, scoring=\"roc_auc\")\n",
        "    test_score_nested.append(test_score.mean())\n",
        "\n",
        "\n",
        "all_scores = {\n",
        "    \"Not nested CV\": test_score_not_nested,\n",
        "    \"Nested CV\": test_score_nested,\n",
        "}\n",
        "all_scores = pd.DataFrame(all_scores)\n",
        "\n",
        "color = {\"whiskers\": \"black\", \"medians\": \"black\", \"caps\": \"black\"}\n",
        "all_scores.plot.box(color=color, vert=True)\n",
        "plt.xlabel(\"AUC\")\n",
        "plt.title(\"Comparison of mean accuracy obtained on the test sets with\\n\"\n",
        "              \"and without nested cross-validation\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFqz3wBC9Slg"
      },
      "source": [
        "## Final task\n",
        "Using the above code, can you implement a scheme to answer the question \"does BMI improve prediction of incident diabetes between 2 and 7 years from onset beyond age, sex, and blood glucose?\".\n",
        "\n",
        "Possible steps:\n",
        "\n",
        "1. Copy the code in the previous section\n",
        "2. Add call to a model with a specified feature subset (as in section Cell 1.3)\n",
        "3. Compare boxplots from these two models"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XuNgNo9of68i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_33fRxC99Slh"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.7 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c2ae599412cc4b41baea9dd763035f91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f3e1a919ad43413581e2ae081a6bf145",
              "IPY_MODEL_2e10c9128dda441b98a89e7c82b6007a",
              "IPY_MODEL_3f162a4dd5274d75b5eb4a27b2379bea"
            ],
            "layout": "IPY_MODEL_37867064a205462a9e48c83857965ccb"
          }
        },
        "f3e1a919ad43413581e2ae081a6bf145": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f570bd61e736466ba935fe1e5d58141d",
            "placeholder": "​",
            "style": "IPY_MODEL_7ac5288e99794e059f3a9d2694e88587",
            "value": "Render widgets:   0%"
          }
        },
        "2e10c9128dda441b98a89e7c82b6007a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fcf493e78bf443c9b6a7b7b07fd3144c",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a8a1daecea794d5d87c3bf445668516c",
            "value": 0
          }
        },
        "3f162a4dd5274d75b5eb4a27b2379bea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1330f754f72c45ceb593d09b35b6492f",
            "placeholder": "​",
            "style": "IPY_MODEL_0d0f52db068f4ceca798a586e438a5fa",
            "value": " 0/1 [00:01&lt;?, ?it/s]"
          }
        },
        "37867064a205462a9e48c83857965ccb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f570bd61e736466ba935fe1e5d58141d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ac5288e99794e059f3a9d2694e88587": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fcf493e78bf443c9b6a7b7b07fd3144c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a8a1daecea794d5d87c3bf445668516c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1330f754f72c45ceb593d09b35b6492f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d0f52db068f4ceca798a586e438a5fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}